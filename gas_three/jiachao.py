# cross_validation_analysis.py
# è¯¦ç»†åˆ†æå’Œå¯è§†åŒ–5æŠ˜äº¤å‰éªŒè¯è¿‡ç¨‹

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import KFold, cross_val_score, cross_validate
from sklearn.cross_decomposition import PLSRegression
from sklearn.metrics import mean_squared_error, r2_score
import seaborn as sns

# è®¾ç½®ä¸­æ–‡å­—ä½“æ”¯æŒ
plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False


class CrossValidationAnalyzer:
    def __init__(self, X, Y, cv_folds=5):
        self.X = X
        self.Y = Y
        self.cv_folds = cv_folds
        self.cv_results = {}

    def detailed_cross_validation(self, n_components_range=range(1, 11)):
        """è¯¦ç»†çš„äº¤å‰éªŒè¯åˆ†æ"""
        print("ğŸ” è¯¦ç»†äº¤å‰éªŒè¯åˆ†æ")
        print("=" * 60)

        # åˆ›å»ºKæŠ˜åˆ†å‰²å™¨
        kf = KFold(n_splits=self.cv_folds, shuffle=True, random_state=42)

        results = {}
        detailed_fold_results = {}

        for n_comp in n_components_range:
            print(f"\nğŸ“Š æµ‹è¯• {n_comp} ä¸ªä¸»æˆåˆ†:")

            pls = PLSRegression(n_components=n_comp)

            # å­˜å‚¨æ¯æŠ˜çš„è¯¦ç»†ç»“æœ
            fold_scores = []
            fold_details = []

            fold_idx = 1
            for train_idx, val_idx in kf.split(self.X):
                # åˆ†å‰²æ•°æ®
                X_train_fold = self.X[train_idx]
                X_val_fold = self.X[val_idx]
                Y_train_fold = self.Y[train_idx]
                Y_val_fold = self.Y[val_idx]

                # è®­ç»ƒå’Œé¢„æµ‹
                pls.fit(X_train_fold, Y_train_fold)
                Y_pred_fold = pls.predict(X_val_fold)

                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                r2_fold = r2_score(Y_val_fold, Y_pred_fold)
                rmse_fold = np.sqrt(mean_squared_error(Y_val_fold, Y_pred_fold))

                fold_scores.append(r2_fold)
                fold_details.append({
                    'fold': fold_idx,
                    'train_samples': len(train_idx),
                    'val_samples': len(val_idx),
                    'r2': r2_fold,
                    'rmse': rmse_fold
                })

                print(f"  Fold {fold_idx}: RÂ²={r2_fold:.4f}, RMSE={rmse_fold:.5f}, "
                      f"Train/Val: {len(train_idx)}/{len(val_idx)}")

                fold_idx += 1

            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            mean_score = np.mean(fold_scores)
            std_score = np.std(fold_scores)

            results[n_comp] = {
                'mean_r2': mean_score,
                'std_r2': std_score,
                'fold_scores': fold_scores,
                'fold_details': fold_details
            }

            print(f"  ğŸ“ˆ å¹³å‡ RÂ²: {mean_score:.4f} Â± {std_score:.4f}")

        self.cv_results = results
        return results

    def visualize_cv_results(self):
        """å¯è§†åŒ–äº¤å‰éªŒè¯ç»“æœ"""
        if not self.cv_results:
            print("âŒ è¯·å…ˆè¿è¡Œ detailed_cross_validation()")
            return

        fig, axes = plt.subplots(2, 3, figsize=(18, 12))

        # å‡†å¤‡æ•°æ®
        n_components = list(self.cv_results.keys())
        mean_scores = [self.cv_results[n]['mean_r2'] for n in n_components]
        std_scores = [self.cv_results[n]['std_r2'] for n in n_components]

        # å›¾1: äº¤å‰éªŒè¯æ›²çº¿
        ax1 = axes[0, 0]
        ax1.errorbar(n_components, mean_scores, yerr=std_scores,
                     marker='o', markersize=8, linewidth=2, capsize=5)
        ax1.fill_between(n_components,
                         np.array(mean_scores) - np.array(std_scores),
                         np.array(mean_scores) + np.array(std_scores),
                         alpha=0.3)

        # æ ‡è®°æœ€ä¼˜ç‚¹
        best_idx = np.argmax(mean_scores)
        best_n = n_components[best_idx]
        ax1.scatter(best_n, mean_scores[best_idx], color='red', s=100,
                    marker='*', label=f'Best: n={best_n}')

        ax1.set_xlabel('Number of Components')
        ax1.set_ylabel('Cross-validation RÂ²')
        ax1.set_title('Cross-validation Performance Curve')
        ax1.grid(True, alpha=0.3)
        ax1.legend()

        # å›¾2: ä¸åŒä¸»æˆåˆ†æ•°çš„å¾—åˆ†åˆ†å¸ƒ
        ax2 = axes[0, 1]

        # å‡†å¤‡ç®±çº¿å›¾æ•°æ®
        all_scores = []
        all_labels = []
        for n_comp in n_components[::2]:  # æ¯éš”ä¸€ä¸ªæ˜¾ç¤ºï¼Œé¿å…è¿‡å¯†
            scores = self.cv_results[n_comp]['fold_scores']
            all_scores.extend(scores)
            all_labels.extend([f'n={n_comp}'] * len(scores))

        # åˆ›å»ºDataFrameç”¨äºseaborn
        df_box = pd.DataFrame({'Components': all_labels, 'RÂ² Score': all_scores})
        sns.boxplot(data=df_box, x='Components', y='RÂ² Score', ax=ax2)
        ax2.set_title('RÂ² Score Distribution by Components')
        ax2.tick_params(axis='x', rotation=45)

        # å›¾3: æ¯æŠ˜è¯¦ç»†è¡¨ç°ï¼ˆçƒ­åŠ›å›¾ï¼‰
        ax3 = axes[0, 2]

        # æ„å»ºçƒ­åŠ›å›¾æ•°æ®
        heatmap_data = []
        for n_comp in n_components:
            fold_scores = self.cv_results[n_comp]['fold_scores']
            heatmap_data.append(fold_scores)

        heatmap_data = np.array(heatmap_data)
        im = ax3.imshow(heatmap_data, cmap='RdYlBu_r', aspect='auto')

        ax3.set_xticks(range(self.cv_folds))
        ax3.set_xticklabels([f'Fold {i + 1}' for i in range(self.cv_folds)])
        ax3.set_yticks(range(len(n_components)))
        ax3.set_yticklabels([f'n={n}' for n in n_components])
        ax3.set_title('RÂ² Scores Heatmap\n(Components vs Folds)')

        # æ·»åŠ æ•°å€¼æ ‡ç­¾
        for i in range(len(n_components)):
            for j in range(self.cv_folds):
                text = ax3.text(j, i, f'{heatmap_data[i, j]:.3f}',
                                ha="center", va="center", color="black", fontsize=8)

        plt.colorbar(im, ax=ax3, label='RÂ² Score')

        # å›¾4: æ–¹å·®åˆ†æ
        ax4 = axes[1, 0]
        variances = [self.cv_results[n]['std_r2'] ** 2 for n in n_components]
        ax4.plot(n_components, variances, 'o-', linewidth=2, markersize=8)
        ax4.set_xlabel('Number of Components')
        ax4.set_ylabel('Variance of RÂ² Scores')
        ax4.set_title('Model Stability Analysis')
        ax4.grid(True, alpha=0.3)

        # å›¾5: è®­ç»ƒé›†å¤§å°å½±å“
        ax5 = axes[1, 1]

        # é€‰æ‹©æœ€ä¼˜ä¸»æˆåˆ†æ•°è¿›è¡Œåˆ†æ
        best_n_comp = n_components[np.argmax(mean_scores)]
        fold_details = self.cv_results[best_n_comp]['fold_details']

        train_sizes = [detail['train_samples'] for detail in fold_details]
        val_scores = [detail['r2'] for detail in fold_details]

        ax5.scatter(train_sizes, val_scores, s=100, alpha=0.7)
        for i, detail in enumerate(fold_details):
            ax5.annotate(f"Fold {detail['fold']}",
                         (detail['train_samples'], detail['r2']),
                         xytext=(5, 5), textcoords='offset points')

        ax5.set_xlabel('Training Set Size')
        ax5.set_ylabel('Validation RÂ²')
        ax5.set_title(f'Training Size vs Performance\n(n_components={best_n_comp})')
        ax5.grid(True, alpha=0.3)

        # å›¾6: æ•°æ®åˆ†å‰²å¯è§†åŒ–
        ax6 = axes[1, 2]

        # åˆ›å»ºåˆ†å‰²å¯è§†åŒ–
        kf = KFold(n_splits=self.cv_folds, shuffle=True, random_state=42)
        fold_data = []

        fold_idx = 1
        for train_idx, val_idx in kf.split(self.X):
            # è®­ç»ƒé›†
            fold_data.extend([(i, fold_idx, 'Train') for i in train_idx])
            # éªŒè¯é›†
            fold_data.extend([(i, fold_idx, 'Validation') for i in val_idx])
            fold_idx += 1

        df_split = pd.DataFrame(fold_data, columns=['Sample_Index', 'Fold', 'Type'])

        # åˆ›å»ºåˆ†å‰²çŸ©é˜µ
        split_matrix = np.zeros((len(self.X), self.cv_folds))
        fold_idx = 0
        for train_idx, val_idx in kf.split(self.X):
            split_matrix[train_idx, fold_idx] = 1  # è®­ç»ƒé›†ä¸º1
            split_matrix[val_idx, fold_idx] = 2  # éªŒè¯é›†ä¸º2
            fold_idx += 1

        im2 = ax6.imshow(split_matrix.T, cmap='RdBu', aspect='auto')
        ax6.set_xlabel('Sample Index')
        ax6.set_ylabel('Fold Number')
        ax6.set_title('K-Fold Data Splitting Visualization')
        ax6.set_yticks(range(self.cv_folds))
        ax6.set_yticklabels([f'Fold {i + 1}' for i in range(self.cv_folds)])

        # æ·»åŠ å›¾ä¾‹
        from matplotlib.patches import Patch
        legend_elements = [Patch(facecolor='blue', label='Training Set'),
                           Patch(facecolor='red', label='Validation Set')]
        ax6.legend(handles=legend_elements, loc='upper right')

        plt.tight_layout()
        plt.savefig('data/figures/cross_validation_analysis.png', dpi=300, bbox_inches='tight')
        print("ğŸ“Š äº¤å‰éªŒè¯åˆ†æå›¾è¡¨å·²ä¿å­˜åˆ°: data/figures/cross_validation_analysis.png")
        plt.show()

    def generate_cv_report(self):
        """ç”Ÿæˆäº¤å‰éªŒè¯è¯¦ç»†æŠ¥å‘Š"""
        if not self.cv_results:
            print("âŒ è¯·å…ˆè¿è¡Œ detailed_cross_validation()")
            return

        from datetime import datetime
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        report_file = f"data/results/cross_validation_report_{timestamp}.txt"

        with open(report_file, 'w', encoding='utf-8') as f:
            f.write("Cross-Validation Detailed Analysis Report\n")
            f.write("=" * 60 + "\n")
            f.write(f"Analysis Time: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"Cross-validation Strategy: {self.cv_folds}-Fold\n")
            f.write(f"Total Samples: {len(self.X)}\n")
            f.write(f"Features: {self.X.shape[1]}\n")
            f.write(f"Target Variables: {self.Y.shape[1]}\n\n")

            f.write("1. Overall Results Summary\n")
            f.write("-" * 40 + "\n")

            best_n = max(self.cv_results.keys(),
                         key=lambda k: self.cv_results[k]['mean_r2'])
            best_score = self.cv_results[best_n]['mean_r2']
            best_std = self.cv_results[best_n]['std_r2']

            f.write(f"Best Configuration:\n")
            f.write(f"  Components: {best_n}\n")
            f.write(f"  Mean RÂ²: {best_score:.6f}\n")
            f.write(f"  Std RÂ²: {best_std:.6f}\n")
            f.write(
                f"  95% Confidence Interval: [{best_score - 1.96 * best_std:.6f}, {best_score + 1.96 * best_std:.6f}]\n\n")

            f.write("2. Detailed Results by Component Number\n")
            f.write("-" * 40 + "\n")
            f.write(f"{'Components':<12}{'Mean RÂ²':<12}{'Std RÂ²':<12}{'Min RÂ²':<12}{'Max RÂ²':<12}\n")
            f.write("-" * 60 + "\n")

            for n_comp in sorted(self.cv_results.keys()):
                result = self.cv_results[n_comp]
                mean_r2 = result['mean_r2']
                std_r2 = result['std_r2']
                min_r2 = min(result['fold_scores'])
                max_r2 = max(result['fold_scores'])

                f.write(f"{n_comp:<12}{mean_r2:<12.6f}{std_r2:<12.6f}{min_r2:<12.6f}{max_r2:<12.6f}\n")

            f.write("\n3. Fold-by-Fold Analysis (Best Configuration)\n")
            f.write("-" * 40 + "\n")

            best_details = self.cv_results[best_n]['fold_details']
            f.write(f"Configuration: {best_n} components\n\n")
            f.write(f"{'Fold':<6}{'Train Samples':<14}{'Val Samples':<12}{'RÂ²':<12}{'RMSE':<12}\n")
            f.write("-" * 56 + "\n")

            for detail in best_details:
                f.write(f"{detail['fold']:<6}{detail['train_samples']:<14}{detail['val_samples']:<12}"
                        f"{detail['r2']:<12.6f}{detail['rmse']:<12.6f}\n")

            f.write("\n4. Statistical Analysis\n")
            f.write("-" * 40 + "\n")

            # è®¡ç®—å˜å¼‚ç³»æ•°
            cv_coefficient = best_std / best_score * 100
            f.write(f"Coefficient of Variation: {cv_coefficient:.2f}%\n")

            # ç¨³å®šæ€§è¯„ä¼°
            if cv_coefficient < 5:
                stability = "Excellent"
            elif cv_coefficient < 10:
                stability = "Good"
            elif cv_coefficient < 15:
                stability = "Acceptable"
            else:
                stability = "Poor"

            f.write(f"Model Stability: {stability}\n")

            # æ€§èƒ½ä¸€è‡´æ€§
            fold_range = max(result['fold_scores']) - min(result['fold_scores'])
            f.write(f"Performance Range: {fold_range:.6f}\n")

            f.write("\n5. Recommendations\n")
            f.write("-" * 40 + "\n")

            if best_score > 0.9:
                f.write("âœ… Excellent model performance, ready for deployment\n")
            elif best_score > 0.8:
                f.write("âœ… Good model performance, suitable for most applications\n")
            elif best_score > 0.7:
                f.write("âš ï¸ Acceptable performance, consider feature engineering\n")
            else:
                f.write("âŒ Poor performance, model needs significant improvement\n")

            if cv_coefficient < 10:
                f.write("âœ… Model shows good stability across folds\n")
            else:
                f.write("âš ï¸ Model shows high variance, consider regularization\n")

        print(f"ğŸ“‹ äº¤å‰éªŒè¯è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")


def analyze_cross_validation():
    """è¿è¡Œå®Œæ•´çš„äº¤å‰éªŒè¯åˆ†æ"""
    print("ğŸ” åŠ è½½æ•°æ®...")

    # åŠ è½½æ•°æ®
    X = pd.read_csv("data/processed/X_dataset.csv").values
    Y = pd.read_csv("data/processed/Y_labels.csv").values

    print(f"æ•°æ®è§„æ¨¡: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")

    # åˆ›å»ºåˆ†æå™¨
    cv_analyzer = CrossValidationAnalyzer(X, Y, cv_folds=5)

    # è¿è¡Œè¯¦ç»†äº¤å‰éªŒè¯
    results = cv_analyzer.detailed_cross_validation()

    # ç”Ÿæˆå¯è§†åŒ–
    cv_analyzer.visualize_cv_results()

    # ç”ŸæˆæŠ¥å‘Š
    cv_analyzer.generate_cv_report()

    print("\nâœ… äº¤å‰éªŒè¯åˆ†æå®Œæˆï¼")
    return cv_analyzer


if __name__ == "__main__":
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    import os

    os.makedirs("data/results", exist_ok=True)
    os.makedirs("data/figures", exist_ok=True)

    # è¿è¡Œåˆ†æ
    analyzer = analyze_cross_validation()