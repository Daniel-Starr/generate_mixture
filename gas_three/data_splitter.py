# data_splitter.py
# æ™ºèƒ½æ•°æ®åˆ†å‰²æ¨¡å— - é¿å…æ•°æ®æ³„éœ²ï¼Œç¡®ä¿æ¨¡å‹è¯„ä¼°çš„å¯é æ€§

import pandas as pd
import numpy as np
from sklearn.model_selection import GroupShuffleSplit
from sklearn.preprocessing import StandardScaler
import matplotlib.pyplot as plt
import seaborn as sns
from typing import Tuple, Optional
import json
import os

class IntelligentDataSplitter:
    """
    æ™ºèƒ½æ•°æ®åˆ†å‰²å™¨ï¼Œç¡®ä¿ï¼š
    1. æµ“åº¦ç»„åˆä¸é‡å ï¼ˆé¿å…æ•°æ®æ³„éœ²ï¼‰
    2. è®­ç»ƒæµ‹è¯•é›†åˆ†å¸ƒç›¸ä¼¼
    3. æ”¯æŒå¤šç§åˆ†å‰²ç­–ç•¥
    """
    
    def __init__(self, test_size: float = 0.2, val_size: float = 0.1, random_state: int = 42):
        self.test_size = test_size
        self.val_size = val_size
        self.random_state = random_state
        self.split_info = {}
        
    def concentration_based_split(self, X: pd.DataFrame, Y: pd.DataFrame, 
                                strategy: str = 'grid') -> Tuple[pd.DataFrame, ...]:
        """
        åŸºäºæµ“åº¦ç»„åˆçš„æ•°æ®åˆ†å‰²
        
        Parameters:
        - strategy: 'grid' (ç½‘æ ¼åˆ†å‰²) æˆ– 'random' (éšæœºåˆ†å‰²)
        """
        print("ğŸ”„ æ‰§è¡Œæ™ºèƒ½æ•°æ®åˆ†å‰²...")
        
        # åˆ›å»ºæµ“åº¦ç»„åˆæ ‡è¯†ç¬¦
        Y_copy = Y.copy()
        Y_copy['combination_id'] = Y_copy.groupby(['NO_conc', 'NO2_conc', 'SO2_conc']).ngroup()
        
        unique_combinations = Y_copy['combination_id'].unique()
        n_unique = len(unique_combinations)
        
        print(f"   ğŸ“Š æ€»è®¡ {n_unique} ä¸ªå”¯ä¸€æµ“åº¦ç»„åˆ")
        
        if strategy == 'grid':
            # ç½‘æ ¼ç­–ç•¥ï¼šç¡®ä¿è®­ç»ƒæµ‹è¯•é›†åœ¨æµ“åº¦ç©ºé—´ä¸­å‡åŒ€åˆ†å¸ƒ
            test_combinations = self._grid_based_selection(Y_copy, unique_combinations)
        else:
            # éšæœºç­–ç•¥ï¼šéšæœºé€‰æ‹©æµ‹è¯•ç»„åˆ
            np.random.seed(self.random_state)
            n_test = max(1, int(n_unique * self.test_size))
            test_combinations = np.random.choice(unique_combinations, size=n_test, replace=False)
        
        # éªŒè¯é›†ç»„åˆï¼ˆä»è®­ç»ƒé›†ä¸­é€‰æ‹©ï¼‰
        remaining_combinations = [c for c in unique_combinations if c not in test_combinations]
        n_val = max(1, int(len(remaining_combinations) * self.val_size / (1 - self.test_size)))
        
        np.random.seed(self.random_state + 1)
        val_combinations = np.random.choice(remaining_combinations, size=min(n_val, len(remaining_combinations)), 
                                          replace=False)
        
        train_combinations = [c for c in remaining_combinations if c not in val_combinations]
        
        # åˆ›å»ºæ•°æ®åˆ†å‰²
        train_mask = Y_copy['combination_id'].isin(train_combinations)
        val_mask = Y_copy['combination_id'].isin(val_combinations)
        test_mask = Y_copy['combination_id'].isin(test_combinations)
        
        X_train = X[train_mask].reset_index(drop=True)
        X_val = X[val_mask].reset_index(drop=True)
        X_test = X[test_mask].reset_index(drop=True)
        
        Y_train = Y[train_mask].reset_index(drop=True)
        Y_val = Y[val_mask].reset_index(drop=True)
        Y_test = Y[test_mask].reset_index(drop=True)
        
        # è®°å½•åˆ†å‰²ä¿¡æ¯
        self.split_info = {
            'strategy': strategy,
            'train_combinations': len(train_combinations),
            'val_combinations': len(val_combinations),
            'test_combinations': len(test_combinations),
            'train_samples': len(X_train),
            'val_samples': len(X_val),
            'test_samples': len(X_test),
            'test_combination_ids': test_combinations.tolist(),
            'val_combination_ids': val_combinations.tolist(),
            'train_combination_ids': train_combinations
        }
        
        self._print_split_summary()
        self._verify_no_overlap(Y_train, Y_val, Y_test)
        
        return X_train, X_val, X_test, Y_train, Y_val, Y_test
    
    def _grid_based_selection(self, Y_copy: pd.DataFrame, unique_combinations: np.ndarray) -> np.ndarray:
        """ç½‘æ ¼ç­–ç•¥ï¼šåœ¨æµ“åº¦ç©ºé—´ä¸­å‡åŒ€é€‰æ‹©æµ‹è¯•æ ·æœ¬"""
        
        # è·å–æ¯ä¸ªç»„åˆçš„ä»£è¡¨æ€§æµ“åº¦
        combination_centers = Y_copy.groupby('combination_id')[['NO_conc', 'NO2_conc', 'SO2_conc']].first()
        
        # ä½¿ç”¨K-meansèšç±»é€‰æ‹©ä»£è¡¨æ€§æµ‹è¯•ç»„åˆ
        from sklearn.cluster import KMeans
        
        n_test_clusters = max(1, int(len(unique_combinations) * self.test_size))
        kmeans = KMeans(n_clusters=n_test_clusters, random_state=self.random_state, n_init=10)
        
        # èšç±»æµ“åº¦ç©ºé—´
        concentration_coords = combination_centers[['NO_conc', 'NO2_conc', 'SO2_conc']].values
        cluster_labels = kmeans.fit_predict(concentration_coords)
        
        # ä»æ¯ä¸ªèšç±»ä¸­é€‰æ‹©æœ€æ¥è¿‘ä¸­å¿ƒçš„ç»„åˆä½œä¸ºæµ‹è¯•é›†
        test_combinations = []
        for cluster_id in range(n_test_clusters):
            cluster_mask = cluster_labels == cluster_id
            cluster_combinations = combination_centers[cluster_mask]
            
            if len(cluster_combinations) > 0:
                cluster_center = kmeans.cluster_centers_[cluster_id]
                # è®¡ç®—åˆ°èšç±»ä¸­å¿ƒçš„è·ç¦»
                distances = np.sum((cluster_combinations[['NO_conc', 'NO2_conc', 'SO2_conc']].values - cluster_center) ** 2, axis=1)
                closest_idx = cluster_combinations.index[np.argmin(distances)]
                test_combinations.append(closest_idx)
        
        return np.array(test_combinations)
    
    def _print_split_summary(self):
        """æ‰“å°åˆ†å‰²æ‘˜è¦"""
        info = self.split_info
        print(f"   âœ… æ•°æ®åˆ†å‰²å®Œæˆ:")
        print(f"      ğŸš‚ è®­ç»ƒé›†: {info['train_combinations']} ç»„åˆ, {info['train_samples']} æ ·æœ¬")
        print(f"      ğŸ” éªŒè¯é›†: {info['val_combinations']} ç»„åˆ, {info['val_samples']} æ ·æœ¬")
        print(f"      ğŸ§ª æµ‹è¯•é›†: {info['test_combinations']} ç»„åˆ, {info['test_samples']} æ ·æœ¬")
        print(f"      ğŸ“Š æ€»ä½“æ¯”ä¾‹: {info['train_samples']/(info['train_samples']+info['val_samples']+info['test_samples']):.1%} / "
              f"{info['val_samples']/(info['train_samples']+info['val_samples']+info['test_samples']):.1%} / "
              f"{info['test_samples']/(info['train_samples']+info['val_samples']+info['test_samples']):.1%}")
    
    def _verify_no_overlap(self, Y_train: pd.DataFrame, Y_val: pd.DataFrame, Y_test: pd.DataFrame):
        """éªŒè¯æ•°æ®é›†é—´æ²¡æœ‰æµ“åº¦ç»„åˆé‡å """
        
        train_combinations = set(Y_train.groupby(['NO_conc', 'NO2_conc', 'SO2_conc']).ngroup().unique())
        val_combinations = set(Y_val.groupby(['NO_conc', 'NO2_conc', 'SO2_conc']).ngroup().unique())
        test_combinations = set(Y_test.groupby(['NO_conc', 'NO2_conc', 'SO2_conc']).ngroup().unique())
        
        train_val_overlap = train_combinations.intersection(val_combinations)
        train_test_overlap = train_combinations.intersection(test_combinations)
        val_test_overlap = val_combinations.intersection(test_combinations)
        
        total_overlap = len(train_val_overlap) + len(train_test_overlap) + len(val_test_overlap)
        
        if total_overlap == 0:
            print(f"   âœ… éªŒè¯é€šè¿‡: æ— æµ“åº¦ç»„åˆé‡å ")
        else:
            print(f"   âš ï¸  è­¦å‘Š: å‘ç° {total_overlap} ä¸ªé‡å ç»„åˆ")
            
        return total_overlap == 0
    
    def visualize_split(self, Y_train: pd.DataFrame, Y_val: pd.DataFrame, Y_test: pd.DataFrame, 
                       save_path: str = "data/figures/data_split_visualization.png"):
        """å¯è§†åŒ–æ•°æ®åˆ†å‰²åœ¨æµ“åº¦ç©ºé—´ä¸­çš„åˆ†å¸ƒ"""
        
        fig, axes = plt.subplots(1, 3, figsize=(18, 5))
        
        # 3Dæ•£ç‚¹å›¾çš„2DæŠ•å½±
        projections = [
            ('NO_conc', 'NO2_conc', 'NO vs NO2'),
            ('NO_conc', 'SO2_conc', 'NO vs SO2'),
            ('NO2_conc', 'SO2_conc', 'NO2 vs SO2')
        ]
        
        colors = ['blue', 'green', 'red']
        labels = ['Train', 'Validation', 'Test']
        datasets = [Y_train, Y_val, Y_test]
        
        for i, (x_col, y_col, title) in enumerate(projections):
            for dataset, color, label in zip(datasets, colors, labels):
                if len(dataset) > 0:
                    axes[i].scatter(dataset[x_col], dataset[y_col], 
                                  c=color, alpha=0.6, s=50, label=label)
            
            axes[i].set_xlabel(x_col.replace('_conc', ' Concentration'))
            axes[i].set_ylabel(y_col.replace('_conc', ' Concentration'))
            axes[i].set_title(title)
            axes[i].legend()
            axes[i].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“Š æ•°æ®åˆ†å‰²å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
    
    def save_split_info(self, save_path: str = "data/processed/split_info.json"):
        """ä¿å­˜åˆ†å‰²ä¿¡æ¯"""
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        
        # è½¬æ¢numpyæ•°ç»„ä¸ºåˆ—è¡¨ä»¥ä¾¿JSONåºåˆ—åŒ–
        save_info = self.split_info.copy()
        for key, value in save_info.items():
            if isinstance(value, np.ndarray):
                save_info[key] = value.tolist()
            elif isinstance(value, np.integer):
                save_info[key] = int(value)
                
        with open(save_path, 'w') as f:
            json.dump(save_info, f, indent=2)
        
        print(f"   ğŸ’¾ åˆ†å‰²ä¿¡æ¯å·²ä¿å­˜åˆ°: {save_path}")

def apply_intelligent_split():
    """åº”ç”¨æ™ºèƒ½æ•°æ®åˆ†å‰²"""
    
    # æ£€æŸ¥æ•°æ®æ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists("data/processed/X_dataset.csv"):
        print("âŒ æ•°æ®æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè¯·å…ˆè¿è¡Œ 03_generate_dataset.py")
        return None
    
    # åŠ è½½æ•°æ®
    X = pd.read_csv("data/processed/X_dataset.csv")
    Y = pd.read_csv("data/processed/Y_labels.csv")
    
    print(f"ğŸ“¥ åŠ è½½æ•°æ®: {X.shape[0]} æ ·æœ¬, {X.shape[1]} ç‰¹å¾")
    
    # åˆ›å»ºåˆ†å‰²å™¨
    splitter = IntelligentDataSplitter(test_size=0.2, val_size=0.1, random_state=42)
    
    # æ‰§è¡Œåˆ†å‰²
    X_train, X_val, X_test, Y_train, Y_val, Y_test = splitter.concentration_based_split(
        X, Y, strategy='grid'
    )
    
    # ä¿å­˜åˆ†å‰²åçš„æ•°æ®
    data_splits = {
        'X_train': X_train, 'Y_train': Y_train,
        'X_val': X_val, 'Y_val': Y_val,
        'X_test': X_test, 'Y_test': Y_test
    }
    
    for name, data in data_splits.items():
        data.to_csv(f"data/processed/{name}.csv", index=False)
    
    print(f"   ğŸ’¾ åˆ†å‰²åæ•°æ®å·²ä¿å­˜åˆ° data/processed/")
    
    # å¯è§†åŒ–å’Œä¿å­˜ä¿¡æ¯
    splitter.visualize_split(Y_train, Y_val, Y_test)
    splitter.save_split_info()
    
    return X_train, X_val, X_test, Y_train, Y_val, Y_test

if __name__ == "__main__":
    apply_intelligent_split()