# run_enhanced_pipeline.py
# è¿è¡Œå¢å¼ºç‰ˆæ°”ä½“å…‰è°±åˆ†ææµæ°´çº¿

import os
import sys
import argparse
import time
from datetime import datetime

def run_complete_pipeline(regenerate_data=True, retrain_model=True):
    """
    è¿è¡Œå®Œæ•´çš„å¢å¼ºæµæ°´çº¿
    
    Parameters:
    - regenerate_data: æ˜¯å¦é‡æ–°ç”Ÿæˆä»¿çœŸæ•°æ®
    - retrain_model: æ˜¯å¦é‡æ–°è®­ç»ƒæ¨¡å‹
    """
    
    print("=" * 60)
    print("ğŸš€ å¢å¼ºç‰ˆæ°”ä½“å…‰è°±åˆ†æç³»ç»Ÿ")
    print("   åŸºäºPLSå›å½’çš„ä¸‰æ°”ä½“(NO, NO2, SO2)æµ“åº¦é¢„æµ‹")
    print("=" * 60)
    
    start_time = time.time()
    
    # ç¡®ä¿å¿…è¦çš„ç›®å½•å­˜åœ¨
    directories = [
        "data/processed", "data/models", "data/figures", 
        "data/raw", "data/results"
    ]
    
    for dir_path in directories:
        os.makedirs(dir_path, exist_ok=True)
        
    print("ğŸ“ ç›®å½•ç»“æ„å·²å‡†å¤‡å°±ç»ª")
    
    try:
        # æ­¥éª¤1: æ•°æ®é¢„å¤„ç†ï¼ˆå¦‚æœéœ€è¦ï¼‰
        if not os.path.exists("data/processed/interpolated_spectra.csv"):
            print("\n" + "="*50)
            print("ğŸ“Š æ­¥éª¤1: å…‰è°±æ•°æ®é¢„å¤„ç†")
            print("="*50)
            
            import subprocess
            result = subprocess.run([sys.executable, "01_preprocess.pyï¼ˆä¸‰æ°”ä½“ç‰ˆï¼‰.py"], 
                                  capture_output=True, text=True)
            if result.returncode != 0:
                print(f"âŒ é¢„å¤„ç†å¤±è´¥: {result.stderr}")
                return False
            print("âœ… å…‰è°±æ•°æ®é¢„å¤„ç†å®Œæˆ")
        
        # æ­¥éª¤2: ç”Ÿæˆå¢å¼ºæ•°æ®é›†
        if regenerate_data or not os.path.exists("data/processed/X_dataset.csv"):
            print("\n" + "="*50)
            print("ğŸ§¬ æ­¥éª¤2: ç”Ÿæˆå¢å¼ºæ··åˆæ•°æ®é›†")
            print("="*50)
            
            import subprocess
            result = subprocess.run([sys.executable, "03_generate_dataset.pyï¼ˆä¸‰æ°”ä½“ç‰ˆï¼‰.py"], 
                                  capture_output=True, text=True)
            if result.returncode != 0:
                print(f"âŒ æ•°æ®ç”Ÿæˆå¤±è´¥: {result.stderr}")
                return False
            else:
                print(result.stdout)
            print("âœ… å¢å¼ºæ··åˆæ•°æ®é›†ç”Ÿæˆå®Œæˆ")
        
        # æ­¥éª¤3: æ™ºèƒ½æ•°æ®åˆ†å‰²
        print("\n" + "="*50)
        print("ğŸ¯ æ­¥éª¤3: æ™ºèƒ½æ•°æ®åˆ†å‰²")
        print("="*50)
        
        from data_splitter import apply_intelligent_split
        split_result = apply_intelligent_split()
        if split_result is None:
            print("âŒ æ•°æ®åˆ†å‰²å¤±è´¥")
            return False
        print("âœ… æ™ºèƒ½æ•°æ®åˆ†å‰²å®Œæˆ")
        
        # æ­¥éª¤4: å¢å¼ºæ¨¡å‹è®­ç»ƒ
        if retrain_model or not os.path.exists("data/models/enhanced_pls_model.pkl"):
            print("\n" + "="*50)
            print("ğŸ§  æ­¥éª¤4: å¢å¼ºæ¨¡å‹è®­ç»ƒ")
            print("="*50)
            
            from enhanced_model_trainer import main as train_main
            trainer, results = train_main()
            
            if trainer is None:
                print("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
                return False
            print("âœ… å¢å¼ºæ¨¡å‹è®­ç»ƒå®Œæˆ")
        
        # æ­¥éª¤5: æ¨¡å‹æ€§èƒ½åˆ†æ
        print("\n" + "="*50)
        print("ğŸ“ˆ æ­¥éª¤5: æ¨¡å‹æ€§èƒ½åˆ†æ")
        print("="*50)
        
        analyze_enhanced_performance()
        print("âœ… æ€§èƒ½åˆ†æå®Œæˆ")
        
        # æ­¥éª¤6: ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        print("\n" + "="*50)
        print("ğŸ“‹ æ­¥éª¤6: ç”Ÿæˆç»¼åˆæŠ¥å‘Š")
        print("="*50)
        
        generate_comprehensive_report()
        print("âœ… ç»¼åˆæŠ¥å‘Šç”Ÿæˆå®Œæˆ")
        
        elapsed_time = time.time() - start_time
        
        print("\n" + "="*60)
        print("ğŸ‰ å¢å¼ºæµæ°´çº¿æ‰§è¡Œå®Œæˆ!")
        print(f"â±ï¸  æ€»è€—æ—¶: {elapsed_time:.2f} ç§’")
        print("="*60)
        
        # æ˜¾ç¤ºå…³é”®ç»“æœ
        display_key_results()
        
        return True
    
    except Exception as e:
        print(f"âŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

def analyze_enhanced_performance():
    """åˆ†æå¢å¼ºæ¨¡å‹çš„æ€§èƒ½"""
    
    import pandas as pd
    import numpy as np
    import json
    import matplotlib.pyplot as plt
    from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
    
    # æ£€æŸ¥å¿…è¦æ–‡ä»¶
    required_files = [
        "data/processed/X_test.csv",
        "data/processed/Y_test.csv", 
        "data/models/enhanced_training_results.json"
    ]
    
    for file_path in required_files:
        if not os.path.exists(file_path):
            print(f"   âš ï¸ ç¼ºå°‘æ–‡ä»¶: {file_path}")
            return
    
    # åŠ è½½æ•°æ®å’Œç»“æœ
    X_test = pd.read_csv("data/processed/X_test.csv")
    Y_test = pd.read_csv("data/processed/Y_test.csv")
    
    with open("data/models/enhanced_training_results.json", 'r') as f:
        results = json.load(f)
    
    # åŠ è½½æ¨¡å‹è¿›è¡Œæµ‹è¯•é›†é¢„æµ‹
    import joblib
    model = joblib.load("data/models/enhanced_pls_model.pkl")
    
    if results['use_scaling']:
        scaler_X = joblib.load("data/models/scaler_X.pkl")
        scaler_Y = joblib.load("data/models/scaler_Y.pkl")
        
        X_test_scaled = scaler_X.transform(X_test)
        Y_test_pred_scaled = model.predict(X_test_scaled)
        Y_test_pred = scaler_Y.inverse_transform(Y_test_pred_scaled)
    else:
        Y_test_pred = model.predict(X_test.values)
    
    # è®¡ç®—æµ‹è¯•é›†æ€§èƒ½
    test_metrics = {
        'test_rmse': float(np.sqrt(mean_squared_error(Y_test.values, Y_test_pred))),
        'test_mae': float(mean_absolute_error(Y_test.values, Y_test_pred)),
        'test_r2': float(r2_score(Y_test.values, Y_test_pred))
    }
    
    # æ¯ä¸ªæ°”ä½“çš„æ€§èƒ½
    gas_names = ['NO', 'NO2', 'SO2']
    for i, gas in enumerate(gas_names):
        test_metrics[f'test_{gas}_rmse'] = float(np.sqrt(mean_squared_error(Y_test.values[:, i], Y_test_pred[:, i])))
        test_metrics[f'test_{gas}_r2'] = float(r2_score(Y_test.values[:, i], Y_test_pred[:, i]))
    
    print("   ğŸ“Š æµ‹è¯•é›†æ€§èƒ½:")
    print(f"      æ•´ä½“ - RMSE: {test_metrics['test_rmse']:.5f}, RÂ²: {test_metrics['test_r2']:.5f}")
    for gas in gas_names:
        print(f"      {gas} - RMSE: {test_metrics[f'test_{gas}_rmse']:.5f}, RÂ²: {test_metrics[f'test_{gas}_r2']:.5f}")
    
    # ä¸åŸå§‹ç®€å•æ¨¡å‹æ¯”è¾ƒ
    comparison = {
        'enhanced_model': test_metrics,
        'improvement_over_original': {
            'rmse_improvement': 'N/A',  # éœ€è¦åŸå§‹ç»“æœè¿›è¡Œæ¯”è¾ƒ
            'r2_improvement': 'N/A'
        }
    }
    
    # ä¿å­˜æ€§èƒ½åˆ†æç»“æœ
    with open("data/results/enhanced_performance_analysis.json", 'w') as f:
        json.dump(comparison, f, indent=2)
    
    print(f"   ğŸ’¾ æ€§èƒ½åˆ†æå·²ä¿å­˜åˆ°: data/results/enhanced_performance_analysis.json")

def generate_comprehensive_report():
    """ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š"""
    
    import json
    import pandas as pd
    from datetime import datetime
    
    report_content = []
    report_content.append("# å¢å¼ºç‰ˆæ°”ä½“å…‰è°±åˆ†æç³»ç»Ÿ - ç»¼åˆæŠ¥å‘Š")
    report_content.append(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    report_content.append("")
    
    # 1. ç³»ç»Ÿæ¦‚è¿°
    report_content.append("## 1. ç³»ç»Ÿæ¦‚è¿°")
    report_content.append("æœ¬ç³»ç»Ÿæ˜¯ä¸€ä¸ªåŸºäºPLSå›å½’çš„ä¸‰æ°”ä½“(NO, NO2, SO2)æµ“åº¦é¢„æµ‹ç³»ç»Ÿï¼Œå…·å¤‡ä»¥ä¸‹æ”¹è¿›:")
    report_content.append("- âœ… è§£å†³äº†æ•°æ®æ³„éœ²é—®é¢˜")
    report_content.append("- âœ… å¢åŠ äº†æ•°æ®å¤æ‚æ€§å’ŒçœŸå®æ€§")
    report_content.append("- âœ… å®ç°äº†ä¸¥æ ¼çš„äº¤å‰éªŒè¯")
    report_content.append("- âœ… æ·»åŠ äº†å¤šå±‚æ¬¡å™ªå£°æ¨¡å‹")
    report_content.append("- âœ… åŒ…å«äº†éçº¿æ€§æ··åˆæ•ˆåº”")
    report_content.append("")
    
    # 2. æ•°æ®ç”Ÿæˆå‚æ•°
    if os.path.exists("data/processed/generation_params.json"):
        with open("data/processed/generation_params.json", 'r') as f:
            gen_params = json.load(f)
        
        report_content.append("## 2. æ•°æ®ç”Ÿæˆå‚æ•°")
        report_content.append(f"- æ€»æ ·æœ¬æ•°: {gen_params.get('total_samples', 'N/A')}")
        report_content.append(f"- å”¯ä¸€æµ“åº¦ç»„åˆ: {gen_params.get('unique_combinations', 'N/A')}")
        report_content.append(f"- åŸºç¡€å™ªå£°æ°´å¹³: {gen_params.get('base_noise_level', 'N/A')}")
        report_content.append(f"- éçº¿æ€§å¼ºåº¦: {gen_params.get('nonlinear_strength', 'N/A')}")
        report_content.append("")
    
    # 3. æ•°æ®åˆ†å‰²ä¿¡æ¯
    if os.path.exists("data/processed/split_info.json"):
        with open("data/processed/split_info.json", 'r') as f:
            split_info = json.load(f)
        
        report_content.append("## 3. æ•°æ®åˆ†å‰²ç­–ç•¥")
        report_content.append(f"- ç­–ç•¥: {split_info.get('strategy', 'N/A')}")
        report_content.append(f"- è®­ç»ƒé›†: {split_info.get('train_samples', 'N/A')} æ ·æœ¬, {split_info.get('train_combinations', 'N/A')} ç»„åˆ")
        report_content.append(f"- éªŒè¯é›†: {split_info.get('val_samples', 'N/A')} æ ·æœ¬, {split_info.get('val_combinations', 'N/A')} ç»„åˆ")
        report_content.append(f"- æµ‹è¯•é›†: {split_info.get('test_samples', 'N/A')} æ ·æœ¬, {split_info.get('test_combinations', 'N/A')} ç»„åˆ")
        report_content.append("- âœ… ç¡®ä¿æµ“åº¦ç»„åˆæ— é‡å ï¼Œé¿å…æ•°æ®æ³„éœ²")
        report_content.append("")
    
    # 4. æ¨¡å‹æ€§èƒ½
    if os.path.exists("data/models/enhanced_training_results.json"):
        with open("data/models/enhanced_training_results.json", 'r') as f:
            model_results = json.load(f)
        
        report_content.append("## 4. æ¨¡å‹æ€§èƒ½")
        report_content.append(f"- æœ€ä¼˜ä¸»æˆåˆ†æ•°: {model_results.get('best_n_components', 'N/A')}")
        
        cv_results = model_results.get('cv_results', {})
        report_content.append(f"- äº¤å‰éªŒè¯RÂ²: {cv_results.get('best_cv_score', 'N/A'):.5f} Â± {cv_results.get('best_cv_std', 'N/A'):.5f}")
        
        val_results = model_results.get('validation_results', {})
        report_content.append(f"- éªŒè¯é›†RMSE: {val_results.get('validation_rmse', 'N/A'):.5f}")
        report_content.append(f"- éªŒè¯é›†RÂ²: {val_results.get('validation_r2', 'N/A'):.5f}")
        report_content.append("")
    
    # 5. æ”¹è¿›æ•ˆæœ
    report_content.append("## 5. å…³é”®æ”¹è¿›")
    report_content.append("### 5.1 æ•°æ®æ³„éœ²é—®é¢˜è§£å†³")
    report_content.append("- åŸé—®é¢˜: è®­ç»ƒæµ‹è¯•é›†æœ‰27/30ä¸ªç›¸åŒæµ“åº¦ç»„åˆ")
    report_content.append("- è§£å†³æ–¹æ¡ˆ: åŸºäºæµ“åº¦ç»„åˆçš„æ™ºèƒ½åˆ†å‰²ï¼Œç¡®ä¿é›¶é‡å ")
    report_content.append("")
    
    report_content.append("### 5.2 æ•°æ®å¤æ‚æ€§å¢å¼º")
    report_content.append("- åŸé—®é¢˜: ä»…30ä¸ªç»„åˆï¼Œ1%ç®€å•å™ªå£°")
    report_content.append("- è§£å†³æ–¹æ¡ˆ: å¤§å¹…å¢åŠ ç»„åˆæ•°ï¼Œå¤šå±‚æ¬¡å™ªå£°æ¨¡å‹")
    report_content.append("")
    
    report_content.append("### 5.3 æ¨¡å‹éªŒè¯ä¸¥æ ¼åŒ–")
    report_content.append("- åŸé—®é¢˜: ç®€å•çš„random_stateåˆ†å‰²")
    report_content.append("- è§£å†³æ–¹æ¡ˆ: ä¸¥æ ¼äº¤å‰éªŒè¯+ç‹¬ç«‹éªŒè¯é›†")
    report_content.append("")
    
    # 6. ä½¿ç”¨æŒ‡å—
    report_content.append("## 6. ä½¿ç”¨æŒ‡å—")
    report_content.append("### 6.1 å¤„ç†çœŸå®æ•°æ®")
    report_content.append("```python")
    report_content.append("from real_data_processor import process_real_data_pipeline")
    report_content.append("result = process_real_data_pipeline('your_data.csv')")
    report_content.append("```")
    report_content.append("")
    
    report_content.append("### 6.2 é‡æ–°è®­ç»ƒæ¨¡å‹")
    report_content.append("```python")
    report_content.append("python run_enhanced_pipeline.py --retrain")
    report_content.append("```")
    report_content.append("")
    
    # ä¿å­˜æŠ¥å‘Š
    report_path = "data/results/comprehensive_report.md"
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write('\n'.join(report_content))
    
    print(f"   ğŸ“‹ ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")

def display_key_results():
    """æ˜¾ç¤ºå…³é”®ç»“æœæ‘˜è¦"""
    
    print("\n" + "ğŸ” å…³é”®ç»“æœæ‘˜è¦:")
    
    # æ•°æ®ç»Ÿè®¡
    if os.path.exists("data/processed/generation_params.json"):
        import json
        with open("data/processed/generation_params.json", 'r') as f:
            params = json.load(f)
        print(f"   ğŸ“Š æ•°æ®é›†: {params.get('total_samples', 'N/A')} æ ·æœ¬, {params.get('unique_combinations', 'N/A')} å”¯ä¸€ç»„åˆ")
    
    # æ¨¡å‹æ€§èƒ½
    if os.path.exists("data/results/enhanced_performance_analysis.json"):
        import json
        with open("data/results/enhanced_performance_analysis.json", 'r') as f:
            perf = json.load(f)
        
        enhanced = perf.get('enhanced_model', {})
        print(f"   ğŸ¯ æµ‹è¯•é›†æ€§èƒ½: RMSE = {enhanced.get('test_rmse', 'N/A'):.5f}, RÂ² = {enhanced.get('test_r2', 'N/A'):.5f}")
    
    # æ–‡ä»¶ä½ç½®
    print("\n   ğŸ“ ä¸»è¦è¾“å‡ºæ–‡ä»¶:")
    important_files = [
        ("æ¨¡å‹æ–‡ä»¶", "data/models/enhanced_pls_model.pkl"),
        ("æ€§èƒ½åˆ†æ", "data/results/enhanced_performance_analysis.json"),
        ("ç»¼åˆæŠ¥å‘Š", "data/results/comprehensive_report.md"),
        ("æ•°æ®åˆ†å‰²å¯è§†åŒ–", "data/figures/data_split_visualization.png"),
        ("æ¨¡å‹æ€§èƒ½å¯è§†åŒ–", "data/figures/enhanced_prediction_results.png")
    ]
    
    for name, path in important_files:
        if os.path.exists(path):
            print(f"      âœ… {name}: {path}")
        else:
            print(f"      âŒ {name}: {path} (ç¼ºå¤±)")

def main():
    """ä¸»å‡½æ•°"""
    
    parser = argparse.ArgumentParser(description='è¿è¡Œå¢å¼ºç‰ˆæ°”ä½“å…‰è°±åˆ†ææµæ°´çº¿')
    parser.add_argument('--regenerate-data', action='store_true', 
                       help='é‡æ–°ç”Ÿæˆä»¿çœŸæ•°æ®')
    parser.add_argument('--retrain', action='store_true',
                       help='é‡æ–°è®­ç»ƒæ¨¡å‹')
    
    args = parser.parse_args()
    
    success = run_complete_pipeline(
        regenerate_data=args.regenerate_data,
        retrain_model=args.retrain
    )
    
    if success:
        print("\nğŸ’¡ ä¸‹ä¸€æ­¥å»ºè®®:")
        print("   1. æŸ¥çœ‹ data/results/comprehensive_report.md äº†è§£è¯¦ç»†ç»“æœ")
        print("   2. ä½¿ç”¨ real_data_processor.process_real_data_pipeline() å¤„ç†æ‚¨çš„çœŸå®æ•°æ®")
        print("   3. æ ¹æ®éœ€è¦è°ƒæ•´æ¨¡å‹å‚æ•°æˆ–æ•°æ®ç”Ÿæˆå‚æ•°")
        
        return 0
    else:
        print("\nâŒ æµæ°´çº¿æ‰§è¡Œå¤±è´¥ï¼Œè¯·æ£€æŸ¥é”™è¯¯ä¿¡æ¯")
        return 1

if __name__ == "__main__":
    sys.exit(main())