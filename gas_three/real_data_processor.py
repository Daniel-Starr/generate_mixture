# real_data_processor.py
# çœŸå®å®éªŒæ•°æ®å¤„ç†æ¨¡å— - å¤„ç†ç”¨æˆ·æä¾›çš„çœŸå®å…‰è°±æ•°æ®

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from scipy import signal
from scipy.interpolate import interp1d
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import json
import os
from typing import Tuple, Optional, Dict, List
import warnings
warnings.filterwarnings('ignore')

class RealDataProcessor:
    """
    çœŸå®æ•°æ®å¤„ç†å™¨ï¼Œæä¾›ï¼š
    1. å¤šç§æ ¼å¼æ•°æ®è¯»å–
    2. å…‰è°±é¢„å¤„ç†å’Œè´¨é‡æ£€æŸ¥
    3. ä¸ä»¿çœŸæ•°æ®çš„å¯¹é½
    4. æ•°æ®è´¨é‡è¯„ä¼°
    """
    
    def __init__(self, reference_wavenumbers_file: str = "data/processed/interpolated_spectra.csv"):
        self.reference_wavenumbers_file = reference_wavenumbers_file
        self.reference_wavenumbers = None
        self.processed_data = {}
        self.quality_metrics = {}
        
        # åŠ è½½å‚è€ƒæ³¢æ•°
        if os.path.exists(reference_wavenumbers_file):
            ref_df = pd.read_csv(reference_wavenumbers_file)
            self.reference_wavenumbers = ref_df['wavenumber'].values
            print(f"ğŸ“ å‚è€ƒæ³¢æ•°èŒƒå›´: {self.reference_wavenumbers.min():.1f} - {self.reference_wavenumbers.max():.1f} cmâ»Â¹")
        
    def load_real_data(self, file_path: str, data_format: str = 'auto', 
                      wavenumber_col: str = 'wavenumber', 
                      intensity_col: str = 'intensity') -> pd.DataFrame:
        """
        åŠ è½½çœŸå®å…‰è°±æ•°æ®
        
        Parameters:
        - file_path: æ•°æ®æ–‡ä»¶è·¯å¾„
        - data_format: 'csv', 'txt', 'excel', 'auto'
        - wavenumber_col: æ³¢æ•°åˆ—å
        - intensity_col: å¼ºåº¦åˆ—å
        """
        print(f"ğŸ“‚ åŠ è½½çœŸå®æ•°æ®: {file_path}")
        
        # è‡ªåŠ¨æ£€æµ‹æ–‡ä»¶æ ¼å¼
        if data_format == 'auto':
            ext = os.path.splitext(file_path)[1].lower()
            if ext == '.csv':
                data_format = 'csv'
            elif ext in ['.txt', '.dat']:
                data_format = 'txt'
            elif ext in ['.xlsx', '.xls']:
                data_format = 'excel'
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶æ ¼å¼: {ext}")
        
        # è¯»å–æ•°æ®
        try:
            if data_format == 'csv':
                df = pd.read_csv(file_path)
            elif data_format == 'txt':
                # å°è¯•ä¸åŒçš„åˆ†éš”ç¬¦
                for sep in ['\t', ' ', ',', ';']:
                    try:
                        df = pd.read_csv(file_path, sep=sep)
                        if df.shape[1] >= 2:
                            break
                    except:
                        continue
                else:
                    raise ValueError("æ— æ³•è§£æTXTæ–‡ä»¶")
            elif data_format == 'excel':
                df = pd.read_excel(file_path)
            
            print(f"   ğŸ“Š æ•°æ®å½¢çŠ¶: {df.shape}")
            print(f"   ğŸ“‹ åˆ—å: {list(df.columns)}")
            
        except Exception as e:
            raise ValueError(f"è¯»å–æ–‡ä»¶å¤±è´¥: {str(e)}")
        
        # éªŒè¯å¿…è¦çš„åˆ—
        if wavenumber_col not in df.columns:
            # å°è¯•è‡ªåŠ¨è¯†åˆ«æ³¢æ•°åˆ—
            possible_wave_cols = ['wavenumber', 'wave', 'wn', 'cm-1', 'frequency']
            for col in possible_wave_cols:
                if col in df.columns.str.lower().values:
                    wavenumber_col = df.columns[df.columns.str.lower() == col][0]
                    break
            else:
                print(f"   âš ï¸ æœªæ‰¾åˆ°æ³¢æ•°åˆ—ï¼Œä½¿ç”¨ç¬¬ä¸€åˆ—: {df.columns[0]}")
                wavenumber_col = df.columns[0]
        
        if intensity_col not in df.columns:
            # å°è¯•è‡ªåŠ¨è¯†åˆ«å¼ºåº¦åˆ—
            possible_intensity_cols = ['intensity', 'absorbance', 'abs', 'transmission', 'signal']
            for col in possible_intensity_cols:
                if col in df.columns.str.lower().values:
                    intensity_col = df.columns[df.columns.str.lower() == col][0]
                    break
            else:
                # ä½¿ç”¨é™¤æ³¢æ•°åˆ—å¤–çš„ç¬¬ä¸€åˆ—
                intensity_cols = [col for col in df.columns if col != wavenumber_col]
                if intensity_cols:
                    intensity_col = intensity_cols[0]
                    print(f"   âš ï¸ æœªæ‰¾åˆ°å¼ºåº¦åˆ—ï¼Œä½¿ç”¨: {intensity_col}")
                else:
                    raise ValueError("æ— æ³•è¯†åˆ«å¼ºåº¦åˆ—")
        
        # æå–æ³¢æ•°å’Œå¼ºåº¦æ•°æ®
        wavenumbers = df[wavenumber_col].values
        intensities = df[intensity_col].values
        
        # åŸºæœ¬æ•°æ®éªŒè¯
        self._validate_spectral_data(wavenumbers, intensities)
        
        # åˆ›å»ºæ ‡å‡†æ ¼å¼çš„DataFrame
        processed_df = pd.DataFrame({
            'wavenumber': wavenumbers,
            'intensity': intensities
        })
        
        # æŒ‰æ³¢æ•°æ’åº
        processed_df = processed_df.sort_values('wavenumber').reset_index(drop=True)
        
        print(f"   âœ… æ•°æ®åŠ è½½æˆåŠŸ")
        print(f"   ğŸ“ æ³¢æ•°èŒƒå›´: {wavenumbers.min():.1f} - {wavenumbers.max():.1f} cmâ»Â¹")
        print(f"   ğŸ“Š å¼ºåº¦èŒƒå›´: {intensities.min():.2e} - {intensities.max():.2e}")
        
        return processed_df
    
    def _validate_spectral_data(self, wavenumbers: np.ndarray, intensities: np.ndarray):
        """éªŒè¯å…‰è°±æ•°æ®è´¨é‡"""
        
        issues = []
        
        # æ£€æŸ¥æ•°æ®é•¿åº¦
        if len(wavenumbers) != len(intensities):
            raise ValueError("æ³¢æ•°å’Œå¼ºåº¦æ•°æ®é•¿åº¦ä¸åŒ¹é…")
        
        if len(wavenumbers) < 100:
            issues.append("æ•°æ®ç‚¹å¤ªå°‘ï¼ˆ<100ï¼‰")
        
        # æ£€æŸ¥NaNå€¼
        if np.isnan(wavenumbers).any():
            issues.append("æ³¢æ•°æ•°æ®åŒ…å«NaNå€¼")
        if np.isnan(intensities).any():
            issues.append("å¼ºåº¦æ•°æ®åŒ…å«NaNå€¼")
        
        # æ£€æŸ¥æ³¢æ•°å•è°ƒæ€§
        if not np.all(np.diff(wavenumbers) > 0):
            issues.append("æ³¢æ•°æ•°æ®éå•è°ƒé€’å¢")
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´åˆç†æ€§
        if wavenumbers.min() < 0 or wavenumbers.max() > 10000:
            issues.append("æ³¢æ•°èŒƒå›´å¼‚å¸¸")
        
        # æ£€æŸ¥å¼ºåº¦å€¼
        if np.all(intensities == 0):
            issues.append("æ‰€æœ‰å¼ºåº¦å€¼ä¸ºé›¶")
        
        if issues:
            print(f"   âš ï¸ æ•°æ®è´¨é‡é—®é¢˜: {'; '.join(issues)}")
        else:
            print(f"   âœ… æ•°æ®è´¨é‡æ£€æŸ¥é€šè¿‡")
    
    def preprocess_spectrum(self, df: pd.DataFrame, 
                          baseline_correction: bool = True,
                          smoothing: bool = True,
                          normalization: str = 'minmax') -> pd.DataFrame:
        """
        å…‰è°±é¢„å¤„ç†
        
        Parameters:
        - baseline_correction: æ˜¯å¦è¿›è¡ŒåŸºçº¿æ ¡æ­£
        - smoothing: æ˜¯å¦å¹³æ»‘
        - normalization: 'minmax', 'std', 'area', None
        """
        print("ğŸ”§ å…‰è°±é¢„å¤„ç†...")
        
        wavenumbers = df['wavenumber'].values
        intensities = df['intensity'].values.copy()
        
        # 1. åŸºçº¿æ ¡æ­£
        if baseline_correction:
            print("   ğŸ“ˆ åŸºçº¿æ ¡æ­£...")
            # ä½¿ç”¨å¤šé¡¹å¼æ‹Ÿåˆå»é™¤åŸºçº¿
            baseline = self._estimate_baseline(wavenumbers, intensities)
            intensities = intensities - baseline
        
        # 2. å¹³æ»‘å¤„ç†
        if smoothing:
            print("   ğŸŒŠ å…‰è°±å¹³æ»‘...")
            # ä½¿ç”¨Savitzky-Golayæ»¤æ³¢
            window_length = min(11, len(intensities) // 10 * 2 + 1)  # ç¡®ä¿ä¸ºå¥‡æ•°
            if window_length >= 3:
                intensities = signal.savgol_filter(intensities, window_length, 3)
        
        # 3. å½’ä¸€åŒ–
        if normalization:
            print(f"   ğŸ“ æ•°æ®å½’ä¸€åŒ– ({normalization})...")
            if normalization == 'minmax':
                intensities = (intensities - intensities.min()) / (intensities.max() - intensities.min())
            elif normalization == 'std':
                intensities = (intensities - intensities.mean()) / intensities.std()
            elif normalization == 'area':
                intensities = intensities / np.trapz(np.abs(intensities), wavenumbers)
        
        # åˆ›å»ºé¢„å¤„ç†åçš„DataFrame
        processed_df = pd.DataFrame({
            'wavenumber': wavenumbers,
            'intensity': intensities
        })
        
        print("   âœ… é¢„å¤„ç†å®Œæˆ")
        
        return processed_df
    
    def _estimate_baseline(self, wavenumbers: np.ndarray, intensities: np.ndarray, 
                          poly_order: int = 3) -> np.ndarray:
        """ä¼°è®¡å…‰è°±åŸºçº¿"""
        
        # ä½¿ç”¨å¤šé¡¹å¼æ‹Ÿåˆä¼°è®¡åŸºçº¿
        # é¦–å…ˆæ‰¾åˆ°å¯èƒ½çš„åŸºçº¿ç‚¹ï¼ˆå±€éƒ¨æœ€å°å€¼ï¼‰
        from scipy.signal import find_peaks
        
        # åè½¬ä¿¡å·æ‰¾å³°ï¼ˆåŸä¿¡å·çš„è°·ï¼‰  
        inverted = -intensities
        peaks, _ = find_peaks(inverted, distance=len(intensities)//20)
        
        if len(peaks) < 3:
            # å¦‚æœå³°å¤ªå°‘ï¼Œä½¿ç”¨è¾¹ç•Œç‚¹å’Œä¸­é—´çš„å‡ ä¸ªç‚¹
            peaks = np.linspace(0, len(intensities)-1, 5).astype(int)
        
        # å¤šé¡¹å¼æ‹ŸåˆåŸºçº¿ç‚¹
        baseline_points = intensities[peaks]
        baseline_wave = wavenumbers[peaks]
        
        poly_coef = np.polyfit(baseline_wave, baseline_points, poly_order)
        baseline = np.polyval(poly_coef, wavenumbers)
        
        return baseline
    
    def align_with_reference(self, df: pd.DataFrame, 
                           interpolation_method: str = 'linear') -> pd.DataFrame:
        """
        å°†å…‰è°±æ•°æ®å¯¹é½åˆ°å‚è€ƒæ³¢æ•°è½´
        """
        if self.reference_wavenumbers is None:
            print("   âš ï¸ æ²¡æœ‰å‚è€ƒæ³¢æ•°ï¼Œè·³è¿‡å¯¹é½")
            return df
        
        print("ğŸ¯ ä¸å‚è€ƒæ³¢æ•°å¯¹é½...")
        
        wavenumbers = df['wavenumber'].values
        intensities = df['intensity'].values
        
        # æ£€æŸ¥é‡å èŒƒå›´
        overlap_min = max(wavenumbers.min(), self.reference_wavenumbers.min())
        overlap_max = min(wavenumbers.max(), self.reference_wavenumbers.max())
        
        if overlap_min >= overlap_max:
            raise ValueError("ä¸å‚è€ƒæ³¢æ•°æ²¡æœ‰é‡å èŒƒå›´")
        
        print(f"   ğŸ“ é‡å èŒƒå›´: {overlap_min:.1f} - {overlap_max:.1f} cmâ»Â¹")
        
        # é™åˆ¶å‚è€ƒæ³¢æ•°åˆ°é‡å èŒƒå›´
        ref_mask = (self.reference_wavenumbers >= overlap_min) & (self.reference_wavenumbers <= overlap_max)
        ref_wavenumbers_aligned = self.reference_wavenumbers[ref_mask]
        
        # æ’å€¼åˆ°å‚è€ƒæ³¢æ•°è½´
        interpolator = interp1d(wavenumbers, intensities, 
                              kind=interpolation_method, 
                              bounds_error=False, fill_value=0)
        
        aligned_intensities = interpolator(ref_wavenumbers_aligned)
        
        # åˆ›å»ºå¯¹é½åçš„DataFrame
        aligned_df = pd.DataFrame({
            'wavenumber': ref_wavenumbers_aligned,
            'intensity': aligned_intensities
        })
        
        print(f"   âœ… å¯¹é½å®Œæˆï¼Œæ•°æ®ç‚¹æ•°: {len(aligned_df)}")
        
        return aligned_df
    
    def assess_data_quality(self, df: pd.DataFrame) -> Dict:
        """è¯„ä¼°æ•°æ®è´¨é‡"""
        
        print("ğŸ“‹ æ•°æ®è´¨é‡è¯„ä¼°...")
        
        wavenumbers = df['wavenumber'].values
        intensities = df['intensity'].values
        
        quality_metrics = {
            'n_points': len(df),
            'wavenumber_range': [float(wavenumbers.min()), float(wavenumbers.max())],
            'wavenumber_resolution': float(np.median(np.diff(wavenumbers))),
            'intensity_range': [float(intensities.min()), float(intensities.max())],
            'intensity_std': float(intensities.std()),
            'signal_to_noise_ratio': float(np.abs(intensities.mean()) / intensities.std()) if intensities.std() > 0 else np.inf,
            'baseline_stability': float(np.std(intensities[:10]) + np.std(intensities[-10:])) / 2,
            'spectral_coverage': 0.0  # å°†åœ¨åé¢è®¡ç®—
        }
        
        # è®¡ç®—å…‰è°±è¦†ç›–ç‡ï¼ˆä¸å‚è€ƒçš„é‡å ç¨‹åº¦ï¼‰
        if self.reference_wavenumbers is not None:
            ref_min, ref_max = self.reference_wavenumbers.min(), self.reference_wavenumbers.max()
            data_min, data_max = wavenumbers.min(), wavenumbers.max()
            
            overlap_min = max(ref_min, data_min)
            overlap_max = min(ref_max, data_max)
            
            if overlap_min < overlap_max:
                ref_range = ref_max - ref_min
                overlap_range = overlap_max - overlap_min
                quality_metrics['spectral_coverage'] = float(overlap_range / ref_range)
        
        # è´¨é‡è¯„çº§
        score = 0
        if quality_metrics['n_points'] >= 1000:
            score += 2
        elif quality_metrics['n_points'] >= 500:
            score += 1
        
        if quality_metrics['signal_to_noise_ratio'] >= 10:
            score += 2
        elif quality_metrics['signal_to_noise_ratio'] >= 5:
            score += 1
        
        if quality_metrics['spectral_coverage'] >= 0.8:
            score += 2
        elif quality_metrics['spectral_coverage'] >= 0.6:
            score += 1
        
        quality_levels = {0: 'Poor', 1: 'Poor', 2: 'Fair', 3: 'Fair', 4: 'Good', 5: 'Good', 6: 'Excellent'}
        quality_metrics['quality_score'] = score
        quality_metrics['quality_level'] = quality_levels[score]
        
        self.quality_metrics = quality_metrics
        
        print(f"   ğŸ“Š æ•°æ®ç‚¹æ•°: {quality_metrics['n_points']}")
        print(f"   ğŸ“ æ³¢æ•°åˆ†è¾¨ç‡: {quality_metrics['wavenumber_resolution']:.2f} cmâ»Â¹")
        print(f"   ğŸ“¶ ä¿¡å™ªæ¯”: {quality_metrics['signal_to_noise_ratio']:.2f}")
        print(f"   ğŸ“ å…‰è°±è¦†ç›–ç‡: {quality_metrics['spectral_coverage']:.1%}")
        print(f"   ğŸ† è´¨é‡è¯„çº§: {quality_metrics['quality_level']} ({score}/6)")
        
        return quality_metrics
    
    def predict_with_model(self, df: pd.DataFrame, 
                          model_path: str = "data/models/enhanced_pls_model.pkl") -> Dict:
        """
        ä½¿ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹é¢„æµ‹æ°”ä½“æµ“åº¦
        """
        print("ğŸ”® ä½¿ç”¨æ¨¡å‹é¢„æµ‹æ°”ä½“æµ“åº¦...")
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")
        
        # åŠ è½½æ¨¡å‹
        model = joblib.load(model_path)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦ç¼©æ”¾å™¨
        scaler_X_path = model_path.replace('enhanced_pls_model.pkl', 'scaler_X.pkl')
        scaler_Y_path = model_path.replace('enhanced_pls_model.pkl', 'scaler_Y.pkl')
        
        use_scaling = os.path.exists(scaler_X_path) and os.path.exists(scaler_Y_path)
        
        if use_scaling:
            scaler_X = joblib.load(scaler_X_path)
            scaler_Y = joblib.load(scaler_Y_path)
        
        # å‡†å¤‡é¢„æµ‹æ•°æ®
        X_real = df['intensity'].values.reshape(1, -1)  # å•ä¸ªæ ·æœ¬
        
        # æ£€æŸ¥ç‰¹å¾ç»´åº¦
        expected_features = model.n_features_in_
        actual_features = X_real.shape[1]
        
        if actual_features != expected_features:
            print(f"   âš ï¸ ç‰¹å¾ç»´åº¦ä¸åŒ¹é…: æœŸæœ› {expected_features}, å®é™… {actual_features}")
            
            if actual_features > expected_features:
                # æˆªå–
                X_real = X_real[:, :expected_features]
                print(f"   âœ‚ï¸ æˆªå–åˆ°å‰ {expected_features} ä¸ªç‰¹å¾")
            else:
                # å¡«å……é›¶å€¼
                padding = np.zeros((1, expected_features - actual_features))
                X_real = np.hstack([X_real, padding])
                print(f"   ğŸ”§ å¡«å……åˆ° {expected_features} ä¸ªç‰¹å¾")
        
        # é¢„æµ‹
        if use_scaling:
            X_real_scaled = scaler_X.transform(X_real)
            Y_pred_scaled = model.predict(X_real_scaled)
            Y_pred = scaler_Y.inverse_transform(Y_pred_scaled)
        else:
            Y_pred = model.predict(X_real)
        
        # ç¡®ä¿æµ“åº¦å’Œä¸º1ä¸”éè´Ÿ
        Y_pred = np.maximum(Y_pred, 0)
        Y_pred = Y_pred / Y_pred.sum()
        
        # æ„å»ºç»“æœ
        gas_names = ['NO', 'NO2', 'SO2']
        predictions = {
            'concentrations': {gas: float(Y_pred[0, i]) for i, gas in enumerate(gas_names)},
            'total_concentration': float(Y_pred.sum()),
            'prediction_confidence': self._estimate_confidence(model, X_real if not use_scaling else X_real_scaled),
        }
        
        print(f"   ğŸ¯ é¢„æµ‹ç»“æœ:")
        for gas, conc in predictions['concentrations'].items():
            print(f"      {gas}: {conc:.3f} ({conc*100:.1f}%)")
        print(f"   ğŸ“Š é¢„æµ‹ç½®ä¿¡åº¦: {predictions['prediction_confidence']:.3f}")
        
        return predictions
    
    def _estimate_confidence(self, model, X: np.ndarray) -> float:
        """ä¼°è®¡é¢„æµ‹ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ç‰ˆï¼‰"""
        
        # åŸºäºæ¨¡å‹çš„æ–¹å·®è§£é‡Šå’Œè¾“å…¥æ•°æ®çš„ç›¸ä¼¼åº¦
        try:
            # è®¡ç®—é¢„æµ‹æ–¹å·®ï¼ˆå¦‚æœæ¨¡å‹æ”¯æŒï¼‰
            if hasattr(model, 'x_scores_'):
                # è®¡ç®—è¾“å…¥åœ¨PLSç©ºé—´ä¸­çš„æŠ•å½±å¼ºåº¦
                scores = model.transform(X)
                max_score = np.max(np.abs(scores))
                
                # åŸºäºåˆ†æ•°çš„ç½®ä¿¡åº¦ï¼ˆç®€åŒ–ï¼‰
                confidence = min(1.0, max_score / 2.0)
                return float(confidence)
        except:
            pass
        
        # é»˜è®¤ä¸­ç­‰ç½®ä¿¡åº¦
        return 0.7
    
    def visualize_processing_steps(self, original_df: pd.DataFrame, 
                                 processed_df: pd.DataFrame,
                                 aligned_df: pd.DataFrame = None,
                                 save_path: str = "data/figures/real_data_processing.png"):
        """å¯è§†åŒ–å¤„ç†æ­¥éª¤"""
        
        print("ğŸ“Š ç”Ÿæˆå¤„ç†æ­¥éª¤å¯è§†åŒ–...")
        
        n_plots = 3 if aligned_df is not None else 2
        fig, axes = plt.subplots(n_plots, 1, figsize=(12, 4*n_plots))
        
        if n_plots == 1:
            axes = [axes]
        
        # åŸå§‹æ•°æ®
        axes[0].plot(original_df['wavenumber'], original_df['intensity'], 'b-', alpha=0.7, linewidth=1)
        axes[0].set_title('åŸå§‹å…‰è°±æ•°æ®')
        axes[0].set_xlabel('Wavenumber (cmâ»Â¹)')
        axes[0].set_ylabel('Intensity')
        axes[0].grid(True, alpha=0.3)
        
        # é¢„å¤„ç†åæ•°æ®
        axes[1].plot(processed_df['wavenumber'], processed_df['intensity'], 'g-', alpha=0.7, linewidth=1)
        axes[1].set_title('é¢„å¤„ç†åå…‰è°±æ•°æ®')
        axes[1].set_xlabel('Wavenumber (cmâ»Â¹)')
        axes[1].set_ylabel('Processed Intensity')
        axes[1].grid(True, alpha=0.3)
        
        # å¯¹é½åæ•°æ®ï¼ˆå¦‚æœæœ‰ï¼‰
        if aligned_df is not None and len(axes) > 2:
            axes[2].plot(aligned_df['wavenumber'], aligned_df['intensity'], 'r-', alpha=0.7, linewidth=1)
            axes[2].set_title('å¯¹é½åå…‰è°±æ•°æ®')
            axes[2].set_xlabel('Wavenumber (cmâ»Â¹)')
            axes[2].set_ylabel('Aligned Intensity')
            axes[2].grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        # ä¿å­˜å›¾ç‰‡
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        plt.savefig(save_path, dpi=300, bbox_inches='tight')
        plt.close()
        
        print(f"   ğŸ“Š å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
    
    def save_processed_data(self, df: pd.DataFrame, 
                           save_path: str = "data/raw/processed_real_data.csv"):
        """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
        
        os.makedirs(os.path.dirname(save_path), exist_ok=True)
        df.to_csv(save_path, index=False)
        
        # ä¿å­˜è´¨é‡è¯„ä¼°ç»“æœ
        if self.quality_metrics:
            quality_path = save_path.replace('.csv', '_quality.json')
            with open(quality_path, 'w') as f:
                json.dump(self.quality_metrics, f, indent=2)
        
        print(f"   ğŸ’¾ å¤„ç†åæ•°æ®å·²ä¿å­˜åˆ°: {save_path}")

def process_real_data_pipeline(file_path: str, 
                             data_format: str = 'auto',
                             wavenumber_col: str = 'wavenumber',
                             intensity_col: str = 'intensity',
                             predict: bool = True) -> Dict:
    """
    çœŸå®æ•°æ®å¤„ç†å®Œæ•´æµæ°´çº¿
    """
    print("ğŸš€ å¼€å§‹çœŸå®æ•°æ®å¤„ç†æµæ°´çº¿...")
    
    # åˆ›å»ºå¤„ç†å™¨
    processor = RealDataProcessor()
    
    # 1. åŠ è½½æ•°æ®
    original_df = processor.load_real_data(file_path, data_format, wavenumber_col, intensity_col)
    
    # 2. é¢„å¤„ç†
    processed_df = processor.preprocess_spectrum(original_df)
    
    # 3. å¯¹é½
    aligned_df = processor.align_with_reference(processed_df)
    
    # 4. è´¨é‡è¯„ä¼°
    quality_metrics = processor.assess_data_quality(aligned_df)
    
    # 5. é¢„æµ‹ï¼ˆå¦‚æœè¦æ±‚ï¼‰
    predictions = None
    if predict:
        try:
            predictions = processor.predict_with_model(aligned_df)
        except Exception as e:
            print(f"   âš ï¸ é¢„æµ‹å¤±è´¥: {str(e)}")
            predictions = None
    
    # 6. å¯è§†åŒ–
    processor.visualize_processing_steps(original_df, processed_df, aligned_df)
    
    # 7. ä¿å­˜ç»“æœ
    processor.save_processed_data(aligned_df)
    
    result = {
        'original_data': original_df,
        'processed_data': processed_df, 
        'aligned_data': aligned_df,
        'quality_metrics': quality_metrics,
        'predictions': predictions
    }
    
    print("ğŸ‰ çœŸå®æ•°æ®å¤„ç†å®Œæˆï¼")
    
    return result

if __name__ == "__main__":
    # ç¤ºä¾‹ç”¨æ³•
    print("ğŸ’¡ çœŸå®æ•°æ®å¤„ç†å™¨å·²å‡†å¤‡å°±ç»ª")
    print("   ä½¿ç”¨æ–¹æ³•: process_real_data_pipeline('your_data_file.csv')")
    print("   æ”¯æŒæ ¼å¼: CSV, TXT, Excel")
    print("   è¯·ç¡®ä¿æ•°æ®åŒ…å«æ³¢æ•°å’Œå¼ºåº¦åˆ—")