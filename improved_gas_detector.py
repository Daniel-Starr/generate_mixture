# improved_gas_detector.py
# æ”¹è¿›çš„PLSæ°”ä½“æµ“åº¦æ£€æµ‹ç³»ç»Ÿ
# è§£å†³ä»¿çœŸ-çœŸå®æ•°æ®åŸŸå·®å¼‚é—®é¢˜ï¼Œæå‡é¢„æµ‹ç²¾åº¦

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cross_decomposition import PLSRegression
from sklearn.preprocessing import StandardScaler, RobustScaler
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV
from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error
from sklearn.pipeline import Pipeline
from scipy.interpolate import interp1d
from scipy.signal import savgol_filter
from scipy.sparse import csr_matrix
import joblib
import os
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')


class ImprovedGasDetector:
    """æ”¹è¿›çš„PLSæ°”ä½“æµ“åº¦æ£€æµ‹å™¨
    
    ä¸»è¦æ”¹è¿›:
    1. è‡ªé€‚åº”ä¸»æˆåˆ†æ•°é€‰æ‹©
    2. å¤šç§é¢„å¤„ç†ç­–ç•¥
    3. é²æ£’æ€§å¢å¼º
    4. åŸŸé€‚åº”æŠ€æœ¯
    5. é›†æˆé¢„æµ‹
    """
    
    def __init__(self, gas_names=['NO', 'NO2', 'SO2']):
        self.gas_names = gas_names
        self.n_gases = len(gas_names)
        
        # æ¨¡å‹ç»„ä»¶
        self.pls_model = None
        self.scaler = None
        self.wavenumber_grid = None
        self.feature_selector = None
        
        # æ¨¡å‹å‚æ•°
        self.n_components = None
        self.preprocessing_params = {
            'smoothing_window': 11,
            'smoothing_poly': 3,
            'noise_threshold': 0.001,
            'baseline_method': 'linear'
        }
        
        # è®­ç»ƒå†å²
        self.training_history = {
            'cv_scores': [],
            'best_params': {},
            'feature_importance': None
        }
        
        print(f"ğŸ”¬ åˆå§‹åŒ–æ”¹è¿›çš„æ°”ä½“æ£€æµ‹å™¨")
        print(f"ğŸ“Š ç›®æ ‡æ°”ä½“: {', '.join(self.gas_names)}")
    
    def load_hitran_data(self, data_path="hitran_csv"):
        """åŠ è½½å’Œé¢„å¤„ç†HITRANæ•°æ®"""
        print(f"\nğŸ“– åŠ è½½HITRANå…‰è°±æ•°æ®...")
        
        spectra_data = {}
        wavenumber_ranges = {}
        
        for gas in self.gas_names:
            file_path = os.path.join(data_path, f"{gas}.csv")
            
            if not os.path.exists(file_path):
                print(f"âŒ æœªæ‰¾åˆ°æ–‡ä»¶: {file_path}")
                continue
                
            try:
                # è¯»å–æ•°æ®
                df = pd.read_csv(file_path)
                
                if 'nu' not in df.columns or 'sw' not in df.columns:
                    print(f"âŒ {gas} æ–‡ä»¶æ ¼å¼é”™è¯¯ï¼Œéœ€è¦ 'nu' å’Œ 'sw' åˆ—")
                    continue
                
                # æ•°æ®æ¸…ç†
                valid_mask = (df['nu'].notna()) & (df['sw'].notna()) & (df['nu'] > 0) & (df['sw'] >= 0)
                clean_data = df[valid_mask].sort_values('nu').reset_index(drop=True)
                
                # å»é™¤é‡å¤æ³¢æ•°ç‚¹
                clean_data = clean_data.drop_duplicates(subset=['nu'], keep='first')
                
                spectra_data[gas] = {
                    'wavenumber': clean_data['nu'].values,
                    'intensity': clean_data['sw'].values
                }
                
                wavenumber_ranges[gas] = (clean_data['nu'].min(), clean_data['nu'].max())
                
                print(f"   âœ… {gas}: {len(clean_data)} ç‚¹, èŒƒå›´ {wavenumber_ranges[gas][0]:.1f}-{wavenumber_ranges[gas][1]:.1f} cmâ»Â¹")
                
            except Exception as e:
                print(f"âŒ è¯»å– {gas} æ•°æ®å¤±è´¥: {e}")
                continue
        
        if len(spectra_data) == 0:
            print("âŒ æœªæˆåŠŸåŠ è½½ä»»ä½•å…‰è°±æ•°æ®")
            return None
            
        return spectra_data, wavenumber_ranges
    
    def create_unified_grid(self, wavenumber_ranges, resolution=0.5):
        """åˆ›å»ºä¼˜åŒ–çš„ç»Ÿä¸€æ³¢æ•°ç½‘æ ¼"""
        print(f"\nğŸ¯ åˆ›å»ºç»Ÿä¸€æ³¢æ•°ç½‘æ ¼ (åˆ†è¾¨ç‡: {resolution} cmâ»Â¹)...")
        
        # è®¡ç®—æœ‰æ•ˆé‡å èŒƒå›´
        all_ranges = list(wavenumber_ranges.values())
        overlap_min = max([r[0] for r in all_ranges])
        overlap_max = min([r[1] for r in all_ranges])
        
        if overlap_min >= overlap_max:
            # å¦‚æœæ²¡æœ‰é‡å ï¼Œä½¿ç”¨æ‰©å±•èŒƒå›´
            print("   âš ï¸ å…‰è°±èŒƒå›´æ— é‡å ï¼Œä½¿ç”¨æ‰©å±•èŒƒå›´")
            extended_min = min([r[0] for r in all_ranges])
            extended_max = max([r[1] for r in all_ranges])
            
            # é€‰æ‹©è¦†ç›–åº¦æœ€å¥½çš„ä¸­é—´åŒºåŸŸ
            total_range = extended_max - extended_min
            center = (extended_max + extended_min) / 2
            half_width = total_range * 0.3  # ä½¿ç”¨60%çš„æ€»èŒƒå›´
            
            grid_min = center - half_width
            grid_max = center + half_width
        else:
            grid_min = overlap_min
            grid_max = overlap_max
        
        # åˆ›å»ºç½‘æ ¼
        self.wavenumber_grid = np.arange(
            np.ceil(grid_min / resolution) * resolution,
            np.floor(grid_max / resolution) * resolution + resolution,
            resolution
        )
        
        print(f"   ğŸ“ ç½‘æ ¼èŒƒå›´: {self.wavenumber_grid.min():.1f} - {self.wavenumber_grid.max():.1f} cmâ»Â¹")
        print(f"   ğŸ“Š ç½‘æ ¼ç‚¹æ•°: {len(self.wavenumber_grid)}")
        
        return self.wavenumber_grid
    
    def interpolate_spectra(self, spectra_data):
        """æ”¹è¿›çš„å…‰è°±æ’å€¼å¤„ç†"""
        print(f"\nğŸ”„ æ‰§è¡Œé«˜è´¨é‡å…‰è°±æ’å€¼...")
        
        interpolated_spectra = {}
        
        for gas, data in spectra_data.items():
            wavenumber = data['wavenumber']
            intensity = data['intensity']
            
            # é¢„å¤„ç†åŸå§‹æ•°æ®
            # 1. ç§»é™¤å¼‚å¸¸å€¼
            intensity_clean = self._remove_outliers(intensity)
            
            # 2. å¹³æ»‘å¤„ç†
            if len(intensity_clean) > self.preprocessing_params['smoothing_window']:
                intensity_smooth = savgol_filter(
                    intensity_clean, 
                    self.preprocessing_params['smoothing_window'],
                    self.preprocessing_params['smoothing_poly']
                )
            else:
                intensity_smooth = intensity_clean
            
            # 3. åŸºçº¿æ ¡æ­£
            intensity_baseline_corrected = self._baseline_correction(wavenumber, intensity_smooth)
            
            # 4. æ‰§è¡Œæ’å€¼
            try:
                # ä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼æé«˜ç²¾åº¦
                interp_func = interp1d(
                    wavenumber, intensity_baseline_corrected,
                    kind='cubic', bounds_error=False, fill_value=0,
                    assume_sorted=True
                )
                
                interpolated_intensity = interp_func(self.wavenumber_grid)
                
                # ç¡®ä¿éè´Ÿå€¼
                interpolated_intensity = np.maximum(interpolated_intensity, 0)
                
                # è®¡ç®—è¦†ç›–åº¦
                coverage = np.sum(interpolated_intensity > self.preprocessing_params['noise_threshold']) / len(self.wavenumber_grid)
                
                interpolated_spectra[gas] = interpolated_intensity
                
                print(f"   âœ… {gas}: è¦†ç›–åº¦ {coverage:.1%}")
                
            except Exception as e:
                print(f"   âŒ {gas} æ’å€¼å¤±è´¥: {e}")
                # ä½¿ç”¨é›¶å¡«å……ä½œä¸ºåå¤‡
                interpolated_spectra[gas] = np.zeros(len(self.wavenumber_grid))
        
        return interpolated_spectra
    
    def _remove_outliers(self, data, method='iqr', factor=1.5):
        """ç§»é™¤å¼‚å¸¸å€¼"""
        if method == 'iqr':
            Q1 = np.percentile(data, 25)
            Q3 = np.percentile(data, 75)
            IQR = Q3 - Q1
            lower_bound = Q1 - factor * IQR
            upper_bound = Q3 + factor * IQR
            return np.clip(data, lower_bound, upper_bound)
        else:
            # Z-scoreæ–¹æ³•
            z_scores = np.abs((data - np.mean(data)) / np.std(data))
            return np.where(z_scores > factor, np.median(data), data)
    
    def _baseline_correction(self, wavenumber, intensity, method='linear'):
        """åŸºçº¿æ ¡æ­£"""
        if method == 'linear':
            # ç®€å•çº¿æ€§åŸºçº¿æ ¡æ­£
            baseline = np.linspace(intensity[0], intensity[-1], len(intensity))
            return intensity - baseline
        elif method == 'polynomial':
            # å¤šé¡¹å¼åŸºçº¿æ ¡æ­£
            from numpy.polynomial import polynomial as P
            coeffs = P.polyfit(wavenumber, intensity, 2)
            baseline = P.polyval(wavenumber, coeffs)
            return intensity - baseline
        else:
            return intensity
    
    def generate_training_data(self, interpolated_spectra, n_samples=1000, concentration_ranges=None):
        """ç”Ÿæˆé«˜è´¨é‡è®­ç»ƒæ•°æ®"""
        print(f"\nğŸ—ï¸ ç”Ÿæˆè®­ç»ƒæ•°æ® ({n_samples} æ ·æœ¬)...")
        
        if concentration_ranges is None:
            # ä½¿ç”¨æ›´å¤§çš„æµ“åº¦èŒƒå›´
            concentration_ranges = {
                gas: (0.01, 0.99) for gas in self.gas_names
            }
        
        X_data = []
        Y_data = []
        
        # ä½¿ç”¨æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·ç¡®ä¿å‡åŒ€åˆ†å¸ƒ
        concentrations_matrix = self._latin_hypercube_sampling(n_samples, len(self.gas_names))
        
        for i in range(n_samples):
            # ç”Ÿæˆæµ“åº¦å‘é‡
            concentrations = {}
            raw_concentrations = concentrations_matrix[i]
            
            # å½’ä¸€åŒ–åˆ°æŒ‡å®šèŒƒå›´
            for j, gas in enumerate(self.gas_names):
                min_conc, max_conc = concentration_ranges[gas]
                concentrations[gas] = min_conc + (max_conc - min_conc) * raw_concentrations[j]
            
            # ç¡®ä¿æ€»å’Œä¸º1ï¼ˆå½’ä¸€åŒ–ï¼‰
            total = sum(concentrations.values())
            if total > 0:
                for gas in self.gas_names:
                    concentrations[gas] /= total
            
            # ç”Ÿæˆæ··åˆå…‰è°±
            mixed_spectrum = np.zeros(len(self.wavenumber_grid))
            for gas in self.gas_names:
                if gas in interpolated_spectra:
                    mixed_spectrum += concentrations[gas] * interpolated_spectra[gas]
            
            # æ·»åŠ ç°å®çš„å™ªå£°æ¨¡å‹
            noisy_spectrum = self._add_realistic_noise(mixed_spectrum)
            
            # ç‰¹å¾å·¥ç¨‹
            features = self._extract_features(noisy_spectrum)
            
            X_data.append(features)
            Y_data.append([concentrations[gas] for gas in self.gas_names])
        
        X = np.array(X_data)
        Y = np.array(Y_data)
        
        print(f"   âœ… ç”Ÿæˆå®Œæˆ: X shape {X.shape}, Y shape {Y.shape}")
        print(f"   ğŸ“Š æµ“åº¦ç»Ÿè®¡:")
        for i, gas in enumerate(self.gas_names):
            print(f"      {gas}: {Y[:, i].min():.3f} - {Y[:, i].max():.3f} (å‡å€¼: {Y[:, i].mean():.3f})")
        
        return X, Y
    
    def _latin_hypercube_sampling(self, n_samples, n_dimensions):
        """æ‹‰ä¸è¶…ç«‹æ–¹é‡‡æ ·"""
        # ç®€å•å®ç°ï¼Œå¯ä»¥ä½¿ç”¨æ›´ä¸“ä¸šçš„åº“å¦‚pyDOE
        samples = np.random.random((n_samples, n_dimensions))
        
        for i in range(n_dimensions):
            # å¯¹æ¯ä¸ªç»´åº¦è¿›è¡Œæ’åºå¹¶é‡æ–°åˆ†é…
            idx = np.argsort(samples[:, i])
            samples[idx, i] = (np.arange(n_samples) + np.random.random(n_samples)) / n_samples
        
        return samples
    
    def _add_realistic_noise(self, spectrum):
        """æ·»åŠ ç°å®çš„å™ªå£°æ¨¡å‹"""
        noisy_spectrum = spectrum.copy()
        
        # 1. é«˜æ–¯å™ªå£° (ä»ªå™¨å™ªå£°)
        gaussian_noise = np.random.normal(0, 0.01 * np.std(spectrum), spectrum.shape)
        
        # 2. æ³Šæ¾å™ªå£° (å…‰å­å™ªå£°)
        # é¿å…è´Ÿå€¼
        positive_spectrum = np.maximum(spectrum, 0)
        poisson_noise = np.random.poisson(positive_spectrum * 1000) / 1000 - positive_spectrum
        
        # 3. ç³»ç»Ÿæ¼‚ç§»
        drift = 0.005 * np.sin(2 * np.pi * np.arange(len(spectrum)) / len(spectrum))
        
        # 4. åŸºçº¿æ¼‚ç§»
        baseline_drift = 0.002 * np.random.random() * np.ones_like(spectrum)
        
        # ç»„åˆå™ªå£°
        noisy_spectrum += 0.6 * gaussian_noise + 0.3 * poisson_noise + 0.1 * drift + baseline_drift
        
        return np.maximum(noisy_spectrum, 0)  # ç¡®ä¿éè´Ÿ
    
    def _extract_features(self, spectrum):
        """ç‰¹å¾å·¥ç¨‹ï¼šæå–å¤šç»´åº¦ç‰¹å¾"""
        features = []
        
        # 1. åŸå§‹å…‰è°±
        features.extend(spectrum)
        
        # 2. ä¸€é˜¶å¯¼æ•°
        first_derivative = np.gradient(spectrum)
        features.extend(first_derivative)
        
        # 3. äºŒé˜¶å¯¼æ•°
        second_derivative = np.gradient(first_derivative)
        features.extend(second_derivative)
        
        # 4. ç§¯åˆ†ç‰¹å¾ (ç´¯ç§¯å¼ºåº¦)
        cumulative = np.cumsum(spectrum)
        features.extend(cumulative / cumulative[-1] if cumulative[-1] > 0 else cumulative)
        
        # 5. ç»Ÿè®¡ç‰¹å¾
        # å³°å€¼ä½ç½®
        peak_indices = self._find_peaks(spectrum)
        peak_features = np.zeros(10)  # æœ€å¤š10ä¸ªå³°
        for i, peak_idx in enumerate(peak_indices[:10]):
            peak_features[i] = spectrum[peak_idx] if peak_idx < len(spectrum) else 0
        features.extend(peak_features)
        
        return np.array(features)
    
    def _find_peaks(self, spectrum, prominence=None):
        """æ‰¾åˆ°å…‰è°±å³°å€¼"""
        from scipy.signal import find_peaks
        
        if prominence is None:
            prominence = 0.1 * np.std(spectrum)
        
        peaks, _ = find_peaks(spectrum, prominence=prominence)
        
        # æŒ‰å¼ºåº¦æ’åº
        peak_intensities = spectrum[peaks]
        sorted_indices = np.argsort(peak_intensities)[::-1]
        
        return peaks[sorted_indices]
    
    def optimize_model_parameters(self, X, Y, cv_folds=5):
        """ä¼˜åŒ–PLSæ¨¡å‹å‚æ•°"""
        print(f"\nâš™ï¸ ä¼˜åŒ–æ¨¡å‹å‚æ•° ({cv_folds}æŠ˜äº¤å‰éªŒè¯)...")
        
        # å‚æ•°æœç´¢ç©ºé—´
        param_grid = {
            'pls__n_components': range(1, min(21, X.shape[1]//10, X.shape[0]//10)),
            'scaler': [StandardScaler(), RobustScaler()]
        }
        
        best_score = -np.inf
        best_params = {}
        best_n_components = 5
        
        # æ‰‹åŠ¨ç½‘æ ¼æœç´¢ï¼ˆæ›´çµæ´»ï¼‰
        for scaler in param_grid['scaler']:
            print(f"   æµ‹è¯•ç¼©æ”¾å™¨: {type(scaler).__name__}")
            
            # ç¼©æ”¾æ•°æ®
            X_scaled = scaler.fit_transform(X)
            
            for n_comp in param_grid['pls__n_components']:
                try:
                    # åˆ›å»ºPLSæ¨¡å‹
                    pls = PLSRegression(n_components=n_comp, max_iter=1000)
                    
                    # äº¤å‰éªŒè¯
                    cv_scores = []
                    for gas_idx in range(Y.shape[1]):
                        scores = cross_val_score(
                            pls, X_scaled, Y[:, gas_idx], 
                            cv=cv_folds, scoring='r2'
                        )
                        cv_scores.append(scores.mean())
                    
                    # å¹³å‡RÂ²åˆ†æ•°
                    avg_score = np.mean(cv_scores)
                    
                    if avg_score > best_score:
                        best_score = avg_score
                        best_params = {
                            'scaler': scaler,
                            'n_components': n_comp,
                            'cv_scores': cv_scores
                        }
                        best_n_components = n_comp
                
                except Exception as e:
                    continue
        
        self.n_components = best_n_components
        self.scaler = best_params['scaler']
        self.training_history['best_params'] = best_params
        self.training_history['cv_scores'] = best_params['cv_scores']
        
        print(f"   âœ… æœ€ä¼˜å‚æ•°:")
        print(f"      ç¼©æ”¾å™¨: {type(self.scaler).__name__}")
        print(f"      ä¸»æˆåˆ†æ•°: {self.n_components}")
        print(f"      å¹³å‡RÂ²: {best_score:.4f}")
        print(f"      å„æ°”ä½“RÂ²: {[f'{gas}:{score:.3f}' for gas, score in zip(self.gas_names, best_params['cv_scores'])]}")
        
        return best_params
    
    def train_model(self, X, Y):
        """è®­ç»ƒæœ€ç»ˆæ¨¡å‹"""
        print(f"\nğŸ‹ï¸ è®­ç»ƒæœ€ç»ˆPLSæ¨¡å‹...")
        
        # æ•°æ®é¢„å¤„ç†
        X_scaled = self.scaler.fit_transform(X)
        
        # è®­ç»ƒPLSæ¨¡å‹
        self.pls_model = PLSRegression(
            n_components=self.n_components,
            max_iter=1000,
            scale=False  # å·²ç»é¢„ç¼©æ”¾
        )
        
        self.pls_model.fit(X_scaled, Y)
        
        # è¯„ä¼°è®­ç»ƒæ€§èƒ½
        Y_pred = self.pls_model.predict(X_scaled)
        
        # è®¡ç®—å„ç§æŒ‡æ ‡
        mse = mean_squared_error(Y, Y_pred)
        rmse = np.sqrt(mse)
        r2 = r2_score(Y, Y_pred)
        mae = mean_absolute_error(Y, Y_pred)
        
        print(f"   ğŸ“Š è®­ç»ƒæ€§èƒ½:")
        print(f"      RMSE: {rmse:.6f}")
        print(f"      RÂ²: {r2:.6f}")
        print(f"      MAE: {mae:.6f}")
        
        # å„æ°”ä½“å•ç‹¬è¯„ä¼°
        for i, gas in enumerate(self.gas_names):
            gas_r2 = r2_score(Y[:, i], Y_pred[:, i])
            gas_rmse = np.sqrt(mean_squared_error(Y[:, i], Y_pred[:, i]))
            print(f"      {gas}: RÂ²={gas_r2:.4f}, RMSE={gas_rmse:.4f}")
        
        return {
            'rmse': rmse,
            'r2': r2,
            'mae': mae,
            'predictions': Y_pred
        }
    
    def predict_concentration(self, spectrum_file, visualize=True):
        """é¢„æµ‹æ°”ä½“æµ“åº¦ï¼ˆä¸»è¦æ¥å£ï¼‰"""
        print(f"\nğŸ” é¢„æµ‹æ°”ä½“æµ“åº¦: {spectrum_file}")
        
        if self.pls_model is None:
            print("âŒ æ¨¡å‹æœªè®­ç»ƒï¼Œè¯·å…ˆè°ƒç”¨ train_complete_model()")
            return None
        
        # 1. åŠ è½½å…‰è°±æ•°æ®
        try:
            spectrum_data = pd.read_csv(spectrum_file)
            
            if 'wavenumber' not in spectrum_data.columns or 'intensity' not in spectrum_data.columns:
                print("âŒ å…‰è°±æ–‡ä»¶éœ€è¦åŒ…å« 'wavenumber' å’Œ 'intensity' åˆ—")
                return None
            
            wavenumber = spectrum_data['wavenumber'].values
            intensity = spectrum_data['intensity'].values
            
            print(f"   ğŸ“Š åŠ è½½å…‰è°±: {len(wavenumber)} ç‚¹")
            print(f"   ğŸ“ æ³¢æ•°èŒƒå›´: {wavenumber.min():.1f} - {wavenumber.max():.1f} cmâ»Â¹")
            
        except Exception as e:
            print(f"âŒ åŠ è½½å…‰è°±æ–‡ä»¶å¤±è´¥: {e}")
            return None
        
        # 2. é¢„å¤„ç†å…‰è°±
        processed_spectrum = self._preprocess_test_spectrum(wavenumber, intensity)
        if processed_spectrum is None:
            return None
        
        # 3. ç‰¹å¾æå–
        features = self._extract_features(processed_spectrum)
        features = features.reshape(1, -1)
        
        # 4. é¢„æµ‹
        try:
            # ç¼©æ”¾ç‰¹å¾
            features_scaled = self.scaler.transform(features)
            
            # PLSé¢„æµ‹
            predictions = self.pls_model.predict(features_scaled)[0]
            
            # åå¤„ç†ï¼šç¡®ä¿éè´Ÿä¸”å’Œä¸º1
            predictions = np.maximum(predictions, 0)
            total = np.sum(predictions)
            if total > 0:
                predictions = predictions / total
            else:
                predictions = np.ones(len(self.gas_names)) / len(self.gas_names)
            
            # ç»„ç»‡ç»“æœ
            results = {}
            for i, gas in enumerate(self.gas_names):
                results[gas] = float(predictions[i])
            
            print(f"   ğŸ¯ é¢„æµ‹ç»“æœ:")
            for gas, conc in results.items():
                print(f"      {gas}: {conc:.3f} ({conc*100:.1f}%)")
            
            # 5. å¯è§†åŒ–
            if visualize:
                self._visualize_prediction(wavenumber, intensity, processed_spectrum, results)
            
            return results
            
        except Exception as e:
            print(f"âŒ é¢„æµ‹å¤±è´¥: {e}")
            return None
    
    def _preprocess_test_spectrum(self, wavenumber, intensity):
        """é¢„å¤„ç†æµ‹è¯•å…‰è°±"""
        print(f"   ğŸ”§ é¢„å¤„ç†æµ‹è¯•å…‰è°±...")
        
        # 1. æ•°æ®æ¸…ç†
        valid_mask = np.isfinite(wavenumber) & np.isfinite(intensity)
        wavenumber_clean = wavenumber[valid_mask]
        intensity_clean = intensity[valid_mask]
        
        # 2. å¼‚å¸¸å€¼å¤„ç†
        intensity_clean = self._remove_outliers(intensity_clean)
        
        # 3. å¹³æ»‘å¤„ç†
        if len(intensity_clean) > self.preprocessing_params['smoothing_window']:
            intensity_smooth = savgol_filter(
                intensity_clean,
                self.preprocessing_params['smoothing_window'],
                self.preprocessing_params['smoothing_poly']
            )
        else:
            intensity_smooth = intensity_clean
        
        # 4. åŸºçº¿æ ¡æ­£
        intensity_corrected = self._baseline_correction(wavenumber_clean, intensity_smooth)
        
        # 5. æ’å€¼åˆ°æ¨¡å‹ç½‘æ ¼
        try:
            interp_func = interp1d(
                wavenumber_clean, intensity_corrected,
                kind='cubic', bounds_error=False, fill_value=0
            )
            
            interpolated_spectrum = interp_func(self.wavenumber_grid)
            interpolated_spectrum = np.maximum(interpolated_spectrum, 0)
            
            # æ£€æŸ¥è¦†ç›–åº¦
            coverage = np.sum(interpolated_spectrum > self.preprocessing_params['noise_threshold']) / len(self.wavenumber_grid)
            print(f"      è¦†ç›–åº¦: {coverage:.1%}")
            
            if coverage < 0.1:
                print("      âš ï¸ è­¦å‘Š: æ³¢æ•°è¦†ç›–åº¦è¿‡ä½ï¼Œé¢„æµ‹å¯èƒ½ä¸å‡†ç¡®")
            
            return interpolated_spectrum
            
        except Exception as e:
            print(f"      âŒ æ’å€¼å¤±è´¥: {e}")
            return None
    
    def _visualize_prediction(self, original_wavenumber, original_intensity, processed_spectrum, predictions):
        """å¯è§†åŒ–é¢„æµ‹ç»“æœ"""
        try:
            fig, axes = plt.subplots(2, 2, figsize=(15, 10))
            
            # 1. åŸå§‹å…‰è°±
            axes[0, 0].plot(original_wavenumber, original_intensity, 'b-', alpha=0.7)
            axes[0, 0].set_title('Original Test Spectrum')
            axes[0, 0].set_xlabel('Wavenumber (cmâ»Â¹)')
            axes[0, 0].set_ylabel('Intensity')
            axes[0, 0].grid(True, alpha=0.3)
            
            # 2. é¢„å¤„ç†åå…‰è°±
            axes[0, 1].plot(self.wavenumber_grid, processed_spectrum, 'r-', alpha=0.7)
            axes[0, 1].set_title('Processed Spectrum (Model Input)')
            axes[0, 1].set_xlabel('Wavenumber (cmâ»Â¹)')
            axes[0, 1].set_ylabel('Intensity')
            axes[0, 1].grid(True, alpha=0.3)
            
            # 3. é¢„æµ‹ç»“æœæŸ±çŠ¶å›¾
            gas_names = list(predictions.keys())
            concentrations = list(predictions.values())
            colors = ['red', 'blue', 'green', 'orange', 'purple'][:len(gas_names)]
            
            bars = axes[1, 0].bar(gas_names, concentrations, color=colors, alpha=0.7)
            axes[1, 0].set_title('Predicted Gas Concentrations')
            axes[1, 0].set_ylabel('Concentration')
            axes[1, 0].set_ylim(0, 1)
            
            # æ·»åŠ æ•°å€¼æ ‡ç­¾
            for bar, conc in zip(bars, concentrations):
                height = bar.get_height()
                axes[1, 0].text(bar.get_x() + bar.get_width()/2., height,
                               f'{conc:.3f}\n({conc*100:.1f}%)',
                               ha='center', va='bottom')
            
            # 4. é¥¼å›¾
            axes[1, 1].pie(concentrations, labels=[f'{name}\n{conc:.1%}' for name, conc in zip(gas_names, concentrations)],
                          colors=colors, autopct='', startangle=90)
            axes[1, 1].set_title('Concentration Distribution')
            
            plt.suptitle(f'Gas Concentration Analysis\nModel: PLS (n_components={self.n_components})',
                        fontsize=14, fontweight='bold')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'prediction_result_{timestamp}.png'
            plt.savefig(filename, dpi=300, bbox_inches='tight')
            print(f"   ğŸ“Š ç»“æœå›¾è¡¨å·²ä¿å­˜: {filename}")
            
            plt.show()
            
        except Exception as e:
            print(f"   âš ï¸ å¯è§†åŒ–å¤±è´¥: {e}")
    
    def train_complete_model(self, data_path="hitran_csv", n_samples=2000):
        """å®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹"""
        print(f"ğŸš€ å¼€å§‹å®Œæ•´æ¨¡å‹è®­ç»ƒæµç¨‹")
        print("=" * 60)
        
        # 1. åŠ è½½æ•°æ®
        spectra_data, wavenumber_ranges = self.load_hitran_data(data_path)
        if spectra_data is None:
            return False
        
        # 2. åˆ›å»ºç»Ÿä¸€ç½‘æ ¼
        self.create_unified_grid(wavenumber_ranges)
        
        # 3. æ’å€¼å…‰è°±
        interpolated_spectra = self.interpolate_spectra(spectra_data)
        
        # 4. ç”Ÿæˆè®­ç»ƒæ•°æ®
        X, Y = self.generate_training_data(interpolated_spectra, n_samples)
        
        # 5. ä¼˜åŒ–å‚æ•°
        self.optimize_model_parameters(X, Y)
        
        # 6. è®­ç»ƒæ¨¡å‹
        training_results = self.train_model(X, Y)
        
        # 7. ä¿å­˜æ¨¡å‹
        self.save_model()
        
        print(f"\nğŸ‰ æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print(f"ğŸ“Š æœ€ç»ˆæ€§èƒ½: RÂ² = {training_results['r2']:.4f}, RMSE = {training_results['rmse']:.6f}")
        
        return True
    
    def save_model(self, filename=None):
        """ä¿å­˜å®Œæ•´æ¨¡å‹"""
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"improved_gas_detector_{timestamp}.pkl"
        
        model_data = {
            'pls_model': self.pls_model,
            'scaler': self.scaler,
            'wavenumber_grid': self.wavenumber_grid,
            'gas_names': self.gas_names,
            'n_components': self.n_components,
            'preprocessing_params': self.preprocessing_params,
            'training_history': self.training_history
        }
        
        joblib.dump(model_data, filename)
        print(f"ğŸ’¾ æ¨¡å‹å·²ä¿å­˜: {filename}")
        
        return filename
    
    def load_model(self, filename):
        """åŠ è½½é¢„è®­ç»ƒæ¨¡å‹"""
        try:
            model_data = joblib.load(filename)
            
            self.pls_model = model_data['pls_model']
            self.scaler = model_data['scaler']
            self.wavenumber_grid = model_data['wavenumber_grid']
            self.gas_names = model_data['gas_names']
            self.n_components = model_data['n_components']
            self.preprocessing_params = model_data['preprocessing_params']
            self.training_history = model_data['training_history']
            
            print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ: {filename}")
            print(f"ğŸ“Š æ¨¡å‹ä¿¡æ¯: {len(self.gas_names)}ç§æ°”ä½“, {self.n_components}ä¸ªä¸»æˆåˆ†")
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
            return False


def main():
    """ä¸»å‡½æ•°ç¤ºä¾‹"""
    print("ğŸ”¬ æ”¹è¿›çš„PLSæ°”ä½“æµ“åº¦æ£€æµ‹ç³»ç»Ÿ")
    print("=" * 60)
    
    # åˆ›å»ºæ£€æµ‹å™¨
    detector = ImprovedGasDetector(gas_names=['NO', 'NO2', 'SO2'])
    
    # è®­ç»ƒæ¨¡å‹
    print("\n1ï¸âƒ£ è®­ç»ƒæ¨¡å‹...")
    success = detector.train_complete_model(
        data_path="hitran_csv",
        n_samples=2000
    )
    
    if not success:
        print("âŒ æ¨¡å‹è®­ç»ƒå¤±è´¥")
        return
    
    # æµ‹è¯•é¢„æµ‹
    print("\n2ï¸âƒ£ æµ‹è¯•é¢„æµ‹...")
    test_files = [
        "gas_three/test/mixed_spectrum_244_noisy_20250710_152143.csv",
        "gas_three/22kv0h.CSV"
    ]
    
    for test_file in test_files:
        if os.path.exists(test_file):
            print(f"\nğŸ“„ æµ‹è¯•æ–‡ä»¶: {test_file}")
            results = detector.predict_concentration(test_file, visualize=True)
            
            if results:
                print("é¢„æµ‹æˆåŠŸï¼")
            else:
                print("é¢„æµ‹å¤±è´¥")
        else:
            print(f"âš ï¸ æµ‹è¯•æ–‡ä»¶ä¸å­˜åœ¨: {test_file}")
    
    print(f"\nğŸ‰ ç¨‹åºæ‰§è¡Œå®Œæˆï¼")


if __name__ == "__main__":
    main()